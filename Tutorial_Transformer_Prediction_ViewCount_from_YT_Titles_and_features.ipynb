{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6c0b7778",
   "metadata": {},
   "source": [
    "# Predicting YouTube Video Views: A Model Training Guide"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cad4271d",
   "metadata": {},
   "source": [
    "-------------------------------------------------------------\n",
    "Notebook created by Marcelo Amaral.\n",
    "Assisted by GPT, a language model developed by OpenAI. Aug 2023\n",
    "-------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "835e2fa5",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a04d4da",
   "metadata": {},
   "source": [
    "In the vast realm of digital content, YouTube stands out as a primary platform for video sharing and consumption. With countless creators uploading videos every day, understanding the dynamics of video popularity becomes crucial. In this tutorial, we will embark on a journey to predict YouTube video view counts based on various metadata, such as video titles and subscriber counts.\n",
    "\n",
    "To accomplish this, we'll leverage a suite of powerful tools and libraries:\n",
    "\n",
    "    Pandas for data manipulation and analysis.\n",
    "    Transformers and Sentence Transformers for harnessing the might of pre-trained models, facilitating our exploration into the world of Natural Language Processing.\n",
    "    Torch to work with neural networks and deep learning.\n",
    "    Scikit-learn for clustering and metric computations, enabling us to understand the underlying patterns in our data.\n",
    "    Langchain to assist with loading and processing text documents.\n",
    "    Matplotlib and Seaborn for visualizing our data and results, offering insights into the intricate patterns of YouTube popularity.\n",
    "    And lastly, our custom qgr_data_processing package to streamline specific data operations tailored for this project.\n",
    "\n",
    "Join us as we delve into the intricacies of YouTube metadata, build predictive models, and uncover the secrets behind video views!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "23c2a67c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import logging\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import ast\n",
    "from datetime import timezone, datetime\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Dropout, BatchNormalization\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from tensorflow.keras.models import load_model\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d254a2dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from packages import qgr_data_processing as qgr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64856f1b",
   "metadata": {},
   "source": [
    "## Processing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0ec607b",
   "metadata": {},
   "source": [
    "Processing the raw data is a crucial step before any analysis or modeling. We'll start by loading our dataset and then preprocess specific columns to make them more manageable and interpretable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c0ed7df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pre-trained SBERT model\n",
    "#sbert_model = SentenceTransformer('sentence-transformers/paraphrase-MiniLM-L6-v2')\n",
    "sbert_model = SentenceTransformer('sentence-transformers/multi-qa-MiniLM-L6-cos-v1') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d96a38ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to convert duration to seconds\n",
    "def duration_to_seconds(duration_str):\n",
    "    \"\"\"Converts ISO 8601 duration format into seconds.\"\"\"\n",
    "    hours = 0\n",
    "    minutes = 0\n",
    "    seconds = 0\n",
    "\n",
    "    # Extract hours, minutes, and seconds\n",
    "    hour_match = re.search(r'(\\d+)H', duration_str)\n",
    "    minute_match = re.search(r'(\\d+)M', duration_str)\n",
    "    second_match = re.search(r'(\\d+)S', duration_str)\n",
    "\n",
    "    if hour_match:\n",
    "        hours = int(hour_match.group(1))\n",
    "    if minute_match:\n",
    "        minutes = int(minute_match.group(1))\n",
    "    if second_match:\n",
    "        seconds = int(second_match.group(1))\n",
    "\n",
    "    return hours * 3600 + minutes * 60 + seconds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "58ca3c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to convert tags to a vector\n",
    "def tags_to_vector(tags, model, embedding_size):\n",
    "    \"\"\"\n",
    "    Convert a list of tags into a single vector by averaging the embeddings of individual tags.\n",
    "    \"\"\"\n",
    "    if not tags:\n",
    "        return [0] * embedding_size\n",
    "    \n",
    "    # Convert each tag to a vector\n",
    "    tag_vectors = [qgr.convertToVector(tag, model, embedding_size) for tag in tags]\n",
    "    \n",
    "    # Average the vectors\n",
    "    avg_vector = np.mean(tag_vectors, axis=0)\n",
    "    \n",
    "    return avg_vector.tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "512bc9fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded 182 records from the Excel file.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tags</th>\n",
       "      <th>duration_seconds</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[]</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[]</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[simulation, simulation hypothesis, self-simul...</td>\n",
       "      <td>81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[Consciousness, artificial intelligence, deep ...</td>\n",
       "      <td>267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[Consciousness, artificial intelligence, deep ...</td>\n",
       "      <td>1599</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                tags  duration_seconds\n",
       "0                                                 []                60\n",
       "1                                                 []                60\n",
       "2  [simulation, simulation hypothesis, self-simul...                81\n",
       "3  [Consciousness, artificial intelligence, deep ...               267\n",
       "4  [Consciousness, artificial intelligence, deep ...              1599"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Processing data\n",
    "# Set up basic logging to catch any errors during data processing\n",
    "logging.basicConfig(level=logging.ERROR)\n",
    "\n",
    "# Load the video data from an Excel file\n",
    "video_table = pd.read_excel('data/YT_data_sample_video_enriched.xlsx')  # Replace with your database; the larger, the better\n",
    "\n",
    "# Check if the data was loaded correctly\n",
    "if video_table.empty:\n",
    "    print(\"No data loaded from Excel file.\")\n",
    "else:\n",
    "    print(f\"Successfully loaded {video_table.shape[0]} records from the Excel file.\")\n",
    "\n",
    "# Convert the duration column to seconds for uniformity and easier calculations\n",
    "video_table['duration_seconds'] = video_table['duration'].apply(duration_to_seconds)\n",
    "\n",
    "# Parse the tags column to extract individual tags\n",
    "# The tags are stored as a string representation of a list, so we use ast.literal_eval to convert them back to lists\n",
    "video_table['tags'] = video_table['tags'].apply(lambda x: ast.literal_eval(x) if pd.notnull(x) else [])\n",
    "\n",
    "# Display the processed columns to confirm our transformations\n",
    "video_table[['tags', 'duration_seconds']].head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "34437156",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['channelId', 'channelTitle', 'description', 'publishedAt', 'videoCount',\n",
      "       'viewCount', 'subscriberCount', 'country', 'customUrl',\n",
      "       'topicCategories', 'madeForKids', 'keywords', 'hasVideoTrending',\n",
      "       'numberVideoTrending'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "channel_table = pd.read_excel('data/YT_data_sample_channel.xlsx')\n",
    "print(channel_table.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "086e3fb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the 'publishedAt' column to datetime format\n",
    "video_table['publishedAt'] = pd.to_datetime(video_table['publishedAt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b6cfbd90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>publishedAt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023-07-24 18:58:59+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2023-07-17 23:17:10+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2023-06-23 23:55:45+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2023-05-17 16:00:00+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2023-05-08 18:03:24+00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                publishedAt\n",
       "0 2023-07-24 18:58:59+00:00\n",
       "1 2023-07-17 23:17:10+00:00\n",
       "2 2023-06-23 23:55:45+00:00\n",
       "3 2023-05-17 16:00:00+00:00\n",
       "4 2023-05-08 18:03:24+00:00"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display the head of the DataFrame with the new column\n",
    "video_table[['publishedAt']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a7e37302",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>publishedAt</th>\n",
       "      <th>days_since_published</th>\n",
       "      <th>publication_month</th>\n",
       "      <th>day_of_week</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023-07-24 18:58:59+00:00</td>\n",
       "      <td>72</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2023-07-17 23:17:10+00:00</td>\n",
       "      <td>78</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2023-06-23 23:55:45+00:00</td>\n",
       "      <td>102</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2023-05-17 16:00:00+00:00</td>\n",
       "      <td>140</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2023-05-08 18:03:24+00:00</td>\n",
       "      <td>149</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                publishedAt  days_since_published  publication_month  \\\n",
       "0 2023-07-24 18:58:59+00:00                    72                  7   \n",
       "1 2023-07-17 23:17:10+00:00                    78                  7   \n",
       "2 2023-06-23 23:55:45+00:00                   102                  6   \n",
       "3 2023-05-17 16:00:00+00:00                   140                  5   \n",
       "4 2023-05-08 18:03:24+00:00                   149                  5   \n",
       "\n",
       "   day_of_week  \n",
       "0            0  \n",
       "1            0  \n",
       "2            4  \n",
       "3            2  \n",
       "4            0  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set current_date to be timezone-aware (UTC)\n",
    "current_date = datetime.now(timezone.utc)\n",
    "\n",
    "# Calculate the number of days since the video was published\n",
    "video_table['days_since_published'] = (current_date - video_table['publishedAt']).dt.days\n",
    "\n",
    "# Extract month and day of week from the 'publishedAt' column\n",
    "video_table['publication_month'] = video_table['publishedAt'].dt.month\n",
    "video_table['day_of_week'] = video_table['publishedAt'].dt.dayofweek  # Monday=0, Sunday=6\n",
    "\n",
    "# Display the head of the DataFrame with the new columns\n",
    "video_table[['publishedAt', 'days_since_published', 'publication_month', 'day_of_week']].head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1f497c51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "print(video_table['tags'].iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b05a6c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a mapping from channelId to subscriberCount\n",
    "channel_subscriber_mapping = dict(zip(channel_table['channelId'], channel_table['subscriberCount']))\n",
    "\n",
    "# Create a DataFrame to store the processed data with additional features\n",
    "processed_video_table_with_features = pd.DataFrame()\n",
    "\n",
    "# Iterate over each row in the video_table DataFrame\n",
    "for _, video in video_table.iterrows():\n",
    "    video_title = video['title']\n",
    "\n",
    "    # Check for meaningful title length\n",
    "    if len(video_title) > 30:\n",
    "        content_vector_element = qgr.convertToVector(video_title, sbert_model, 384)\n",
    "        tags_vector_element = tags_to_vector(video['tags'], sbert_model, 384)\n",
    "        \n",
    "        viewCount = video['viewCount']  # Directly extract the value\n",
    "        days_since_published = video['days_since_published']\n",
    "        duration_seconds = video['duration_seconds']\n",
    "        publication_month = video['publication_month']\n",
    "        day_of_week = video['day_of_week']\n",
    "\n",
    "        # Get subscriberCount using channelId\n",
    "        subscriberCount = channel_subscriber_mapping.get(video['channelId'], np.nan)  # Use np.nan as default if channelId not found\n",
    "\n",
    "        # Create a video row with all the features\n",
    "        video_row = {\n",
    "            'videoId': video['videoId'],\n",
    "            'viewCount': viewCount,\n",
    "            'title': video_title,\n",
    "            'content_vector': content_vector_element,\n",
    "            'tags_vector': tags_vector_element,\n",
    "            'days_since_published': days_since_published,\n",
    "            'duration_seconds': duration_seconds,\n",
    "            'publication_month': publication_month,\n",
    "            'day_of_week': day_of_week,\n",
    "            'subscriberCount': subscriberCount  # Add subscriberCount here\n",
    "        }\n",
    "        \n",
    "        new_row = pd.DataFrame([video_row])\n",
    "        processed_video_table_with_features = pd.concat([processed_video_table_with_features, new_row], ignore_index=True)\n",
    "\n",
    "# Save the newly processed table to an Excel file\n",
    "processed_video_table_with_features.to_excel('data/YT_data_sample_video_enriched_vector_features.xlsx', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "54be4f99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       videoId  viewCount                                              title  \\\n",
      "0  EzwQOdg8Tl4       2326     Exploring The Unification of Spirit and Matter   \n",
      "1  cE8J0g6l6xE       3968              Is Consciousness Connected Over Time?   \n",
      "2  jqxYa8fktEc       3265  Exploring The Self-Simulation Hypothesis & Nat...   \n",
      "3  2UiYlwHS8LI      10679  Trailer for Klee Irwin's \"Are We In A Simulati...   \n",
      "4  tY7bkxxwhqE       1562  كلي إيروين - هل نحن في محاكاة؟ - الجزء 4 - فرض...   \n",
      "\n",
      "                                      content_vector  \\\n",
      "0  [-0.01433632243424654, 0.031085586175322533, -...   \n",
      "1  [0.016870219260454178, -0.040848784148693085, ...   \n",
      "2  [-0.025478752329945564, -0.0526980496942997, -...   \n",
      "3  [-0.09087289869785309, -0.09601858258247375, -...   \n",
      "4  [0.05595903471112251, 0.056695710867643356, -0...   \n",
      "\n",
      "                                         tags_vector  days_since_published  \\\n",
      "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...                    72   \n",
      "1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...                    78   \n",
      "2  [-0.014484544961967251, 0.0010104427532165903,...                   102   \n",
      "3  [-0.008085306370048784, -0.0034684393322095275...                   140   \n",
      "4  [-0.008085306370048784, -0.0034684393322095275...                   149   \n",
      "\n",
      "   duration_seconds  publication_month  day_of_week  subscriberCount  \n",
      "0                60                  7            0           227000  \n",
      "1                60                  7            0           227000  \n",
      "2                81                  6            4           227000  \n",
      "3               267                  5            2           227000  \n",
      "4              1599                  5            0           227000  \n",
      "Total number of rows: 168\n"
     ]
    }
   ],
   "source": [
    "processed_video_table = pd.read_excel('data/YT_data_sample_video_enriched_vector_features.xlsx')\n",
    "print(processed_video_table.head())\n",
    "print(\"Total number of rows:\", processed_video_table.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8761c8bf",
   "metadata": {},
   "source": [
    "The test_size and random_state parameters used in the train_test_split function have specific meanings:\n",
    "\n",
    "    test_size: This parameter determines the proportion of the dataset to be used as the test set. When set to 0.2, it means that 20% of the data will be set aside for testing, and the remaining 80% will be used for training. You can adjust this value depending on how you want to split your data. For example, if you set test_size to 0.3, it would split the data into 70% for training and 30% for testing.\n",
    "\n",
    "    random_state: This parameter is used to control the shuffling of the data before splitting it into training and testing sets. If you use the same integer value for random_state every time you run your code (such as 42), the split will be deterministic, and you'll get the same training and testing sets each time. This can be helpful for reproducibility, especially if you want to share your code or compare results across different runs. If you don't set random_state or set it to None, the split will be different each time you run the code.\n",
    "\n",
    "In summary, test_size=0.2 means that 20% of the data will be used for testing, and random_state=42 ensures that the splitting is reproducible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7e705055",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean view count: 66002.89285714286\n"
     ]
    }
   ],
   "source": [
    "# Convert the 'viewCount' column to numeric, setting errors='coerce' to handle non-numeric values\n",
    "processed_video_table['viewCount'] = pd.to_numeric(processed_video_table['viewCount'], errors='coerce')\n",
    "\n",
    "# You can decide to fill NaN values with a specific number, like 0, or drop the rows\n",
    "processed_video_table['viewCount'].fillna(0, inplace=True)\n",
    "# Alternatively, to drop rows with NaN values, uncomment the following line:\n",
    "# processed_video_table.dropna(subset=['viewCount'], inplace=True)\n",
    "\n",
    "# Now you can calculate the mean\n",
    "mean_view_count = processed_video_table['viewCount'].mean()\n",
    "print(f\"Mean view count: {mean_view_count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3d4a4e81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25th percentile: 1765.25\n",
      "50th percentile: 3385.0\n",
      "75th percentile: 9162.75\n",
      "90th percentile: 29237.30000000005\n",
      "95th percentile: 59945.100000000086\n"
     ]
    }
   ],
   "source": [
    "# Extract the view counts\n",
    "view_counts = processed_video_table['viewCount'].values\n",
    "\n",
    "# Plot a histogram\n",
    "plt.hist(view_counts, bins=50, log=True)  # Using log scale for better visualization if there are outliers\n",
    "plt.xlabel('View Count')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of View Counts')\n",
    "plt.show()\n",
    "\n",
    "# Calculate percentiles\n",
    "percentiles = [25, 50, 75, 90, 95]\n",
    "for p in percentiles:\n",
    "    value = np.percentile(view_counts, p)\n",
    "    print(f\"{p}th percentile: {value}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d13925cf",
   "metadata": {},
   "source": [
    "### model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "481edbdf",
   "metadata": {},
   "source": [
    "The percentiles you've calculated give you a good understanding of the distribution of view counts in your dataset. Here's a summary:\n",
    "\n",
    "    25% of the videos have 24,449 views or fewer.\n",
    "    50% (the median) have 105,342 views or fewer.\n",
    "    75% have 406,981 views or fewer.\n",
    "    90% have 1,780,091 views or fewer.\n",
    "    95% have 4,056,272 views or fewer.\n",
    "\n",
    "These statistics can help you set a meaningful threshold for categorizing videos as \"popular\" or not, depending on the context of your analysis or application. For example, you might choose the 75th percentile as a threshold, meaning that videos with more than 406,981 views would be considered popular."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "fc290249",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(row):\n",
    "    # Convert the string representation of the list to an actual list\n",
    "    content_vector = np.array(ast.literal_eval(row['content_vector']))\n",
    "    tags_vector = np.array(ast.literal_eval(row['tags_vector']))\n",
    "    \n",
    "    days_since_published = np.array([row['days_since_published']])\n",
    "    duration_seconds = np.array([row['duration_seconds']])\n",
    "    publication_month = np.array([row['publication_month']])\n",
    "    day_of_week = np.array([row['day_of_week']])\n",
    "    \n",
    "    # Extract subscriberCount\n",
    "    subscriberCount = np.array([row['subscriberCount']])\n",
    "    \n",
    "    return np.concatenate([content_vector, tags_vector, days_since_published, duration_seconds, publication_month, day_of_week, subscriberCount])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4f428ec9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(168, 773)\n"
     ]
    }
   ],
   "source": [
    "# Using the extract_features function to create a feature matrix\n",
    "X = np.array(processed_video_table.apply(extract_features, axis=1).tolist())\n",
    "\n",
    "# Verify the shape\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6435357d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the features and target\n",
    "y = processed_video_table['viewCount'].values\n",
    "\n",
    "# Convert the features to a proper 2D array shape\n",
    "#X = np.array(X)\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f30c1089",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(134, 773)\n",
      "(34, 773)\n",
      "(134,)\n",
      "(34,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16015134",
   "metadata": {},
   "source": [
    "Feed-Forward Neural Network (also known as a Dense Neural Network) using TensorFlow and Keras. It's a more appropriate choice than the LSTM if the data isn't sequential in nature.\n",
    "\n",
    "Here are some observations and suggestions:\n",
    "\n",
    "    Activation Functions: You're using ReLU (Rectified Linear Unit) activation functions for the dense layers, which is a good choice. ReLU is a common activation function for feed-forward neural networks due to its simplicity and efficiency.\n",
    "\n",
    "    Dropout & Batch Normalization: Dropout helps in regularizing the network and preventing overfitting. Batch normalization can help in faster convergence and can sometimes act as a regularizer.\n",
    "\n",
    "    Model Architecture: The architecture seems reasonable for a start. The model starts with 128 neurons and gradually reduces the neuron count in subsequent layers. Depending on the results, you can adjust the model's complexity.\n",
    "\n",
    "    Callbacks: You've continued using the same set of callbacks - early stopping, model checkpointing, and reducing learning rate on plateau. These are excellent choices and should help in obtaining a better-trained model.\n",
    "\n",
    "    Training: You're training the model with the same setup, which is good. Depending on the results, you might adjust the batch size or even the learning rate.\n",
    "\n",
    "    Evaluation: You're still using mean absolute error (MAE) as the loss function, which is appropriate for regression tasks. After training, you're evaluating the model on the test set.\n",
    "\n",
    "    Saving the Model: You've added code to save the trained model, which is a good practice. This way, you can load the model later without having to retrain it.\n",
    "\n",
    "Suggestions:\n",
    "\n",
    "    Data Preprocessing: Ensure that the features are normalized or standardized before feeding them into the neural network. Neural networks tend to perform better with normalized data.\n",
    "\n",
    "    Model Complexity: Depending on the results, you might want to adjust the model's complexity. If you notice overfitting, consider reducing the number of neurons or layers. Conversely, if the model underfits, you can increase its complexity.\n",
    "\n",
    "    Learning Rate: Sometimes, adjusting the learning rate can lead to better performance. You might want to experiment with different learning rates or even use a learning rate schedule.\n",
    "\n",
    "    Visualize Training Progress: Plotting the training and validation loss (and accuracy if you have it) over epochs can provide valuable insights into the training process.\n",
    "\n",
    "Your current setup looks good for an initial run. After training, based on the results and the visualizations, you can further tweak and optimize the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "de95c58a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply log transformation to the target variable (view counts)\n",
    "y_train_log = np.log1p(y_train)\n",
    "y_test_log = np.log1p(y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d89faf01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-04 15:49:51.926545: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-10-04 15:49:51.926958: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1960] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 1s 54ms/step - loss: 7.8936 - val_loss: 1.9046 - lr: 0.0010\n",
      "Epoch 2/100\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 7.5818 - val_loss: 2.7470 - lr: 0.0010\n",
      "Epoch 3/100\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 7.2411 - val_loss: 2.9924 - lr: 0.0010\n",
      "Epoch 4/100\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 6.8555 - val_loss: 2.9320 - lr: 0.0010\n",
      "Epoch 5/100\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 6.3024 - val_loss: 2.7210 - lr: 0.0010\n",
      "Epoch 6/100\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 5.8854 - val_loss: 1.8841 - lr: 0.0010\n",
      "Epoch 7/100\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 5.2943"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mamaral/.local/lib/python3.8/site-packages/keras/src/engine/training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 0s 15ms/step - loss: 5.2508 - val_loss: 1.0963 - lr: 0.0010\n",
      "Epoch 8/100\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 4.5703 - val_loss: 1.4559 - lr: 0.0010\n",
      "Epoch 9/100\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 3.7980 - val_loss: 2.0240 - lr: 0.0010\n",
      "Epoch 10/100\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 3.0186 - val_loss: 3.0071 - lr: 0.0010\n",
      "Epoch 11/100\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 2.3279 - val_loss: 4.0805 - lr: 0.0010\n",
      "Epoch 12/100\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 1.7326 - val_loss: 5.0956 - lr: 0.0010\n",
      "Epoch 13/100\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 1.7076 - val_loss: 4.4154 - lr: 1.0000e-04\n",
      "Epoch 14/100\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 1.7581 - val_loss: 3.7934 - lr: 1.0000e-04\n",
      "Epoch 15/100\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 1.7375 - val_loss: 3.2461 - lr: 1.0000e-04\n",
      "Epoch 16/100\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 1.6689 - val_loss: 2.7653 - lr: 1.0000e-04\n",
      "Epoch 17/100\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 1.6241 - val_loss: 2.3394 - lr: 1.0000e-04\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.4974\n",
      "Mean Absolute Error: 2.497357130050659\n"
     ]
    }
   ],
   "source": [
    "# simpler architecture like feed-forward neural networks.\n",
    "\n",
    "model = Sequential([\n",
    "    Dense(128, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "    Dropout(0.3), # Add dropout\n",
    "    BatchNormalization(), # Add batch normalization\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(1)\n",
    "])\n",
    "model.compile(optimizer='adam', loss='mean_absolute_error')\n",
    "\n",
    "# Early stopping to monitor the validation loss and stop training when it stops improving\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "# Model checkpointing to save the model with the best validation loss\n",
    "model_checkpoint = ModelCheckpoint('best_model.h5', monitor='val_loss', save_best_only=True)\n",
    "\n",
    "# Reduce learning rate when the validation loss has stopped improving\n",
    "reduce_lr_on_plateau = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=5)\n",
    "\n",
    "\n",
    "# Train the model with the callbacks\n",
    "model.fit(\n",
    "    X_train, y_train_log,\n",
    "    epochs=100, # Increase the number of epochs\n",
    "    batch_size=32,\n",
    "    validation_split=0.1,\n",
    "    callbacks=[early_stopping, model_checkpoint, reduce_lr_on_plateau]\n",
    ")\n",
    "\n",
    "# Evaluate the model\n",
    "loss = model.evaluate(X_test, y_test_log)\n",
    "print(f\"Mean Absolute Error: {loss}\")\n",
    "\n",
    "# Save model\n",
    "model_path = \"data/my_model_yt_features_FFNN_test.h5\"\n",
    "model.save(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "6b8de154",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 2ms/step\n",
      "Mean Absolute Error on original scale: 1.2425944913671862e+25\n"
     ]
    }
   ],
   "source": [
    "# Predict on the test set\n",
    "y_pred_log = model.predict(X_test)\n",
    "\n",
    "# Apply the inverse transformation to the predictions\n",
    "y_pred = np.expm1(y_pred_log)\n",
    "\n",
    "# Compute the MAE on the original scale\n",
    "mae_original_scale = mean_absolute_error(y_test, y_pred.flatten())\n",
    "\n",
    "print(f\"Mean Absolute Error on original scale: {mae_original_scale}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cac0b390",
   "metadata": {},
   "source": [
    "A baseline comparison is used to provide a reference point for evaluating the performance of your model. It helps you understand whether the complexity of your model is providing any benefit compared to simpler, heuristic-based approaches. Common baseline models include predicting the mean, median, or mode of the target variable for all observations.\n",
    "\n",
    "Here's how you can create a baseline comparison using the mean and median of the view counts:\n",
    "\n",
    "    Mean Baseline:\n",
    "        Compute the mean of the training target variable (y_train).\n",
    "        Create predictions for the test set by assigning this mean value to all the observations.\n",
    "        Compute the Mean Absolute Error (MAE) between these constant predictions and the actual values in the test set (y_test).\n",
    "\n",
    "    Median Baseline:\n",
    "        Similar to the mean baseline, but using the median of the training target variable instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "f79d98b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Baseline MAE: 86577.91176470589\n",
      "Median Baseline MAE: 31605.91176470588\n"
     ]
    }
   ],
   "source": [
    "# Mean Baseline\n",
    "mean_view_count = np.mean(y_train)\n",
    "mean_predictions = np.full_like(y_test, mean_view_count)\n",
    "mean_baseline_mae = mean_absolute_error(y_test, mean_predictions)\n",
    "print(f\"Mean Baseline MAE: {mean_baseline_mae}\")\n",
    "\n",
    "# Median Baseline\n",
    "median_view_count = np.median(y_train)\n",
    "median_predictions = np.full_like(y_test, median_view_count)\n",
    "median_baseline_mae = mean_absolute_error(y_test, median_predictions)\n",
    "print(f\"Median Baseline MAE: {median_baseline_mae}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4e9ed75",
   "metadata": {},
   "source": [
    "For data ~100K videos:\n",
    "The results indicate that:\n",
    "\n",
    "    Model's MAE on the original scale: 853,336.44853,336.44\n",
    "    Mean Baseline MAE: 1,216,206.791,216,206.79\n",
    "    Median Baseline MAE: 848,345.63848,345.63\n",
    "\n",
    "Your neural network model's performance (MAE of 853,336.44853,336.44) is significantly better than the mean baseline (a basic model that always predicts the mean view count of the training set) but is quite close to the median baseline.\n",
    "\n",
    "Here's a breakdown of the results:\n",
    "\n",
    "    The mean baseline simply predicts the average view count for all videos. Given its MAE, it's off by about 1.21.2 million views on average.\n",
    "\n",
    "    The median baseline predicts the median view count for all videos. It's off by about 848,346848,346 views on average, which is very close to the neural network's MAE.\n",
    "\n",
    "    The neural network's MAE is slightly better than the median baseline, but the improvement might not be significant enough to justify the complexity of the neural network model over a simple median predictor.\n",
    "\n",
    "Here's what you might consider:\n",
    "\n",
    "    Model Complexity vs. Benefit: If the neural network's improvement over the median baseline is marginal, you might weigh the benefits of the slightly improved accuracy against the complexity and computational cost of training and deploying the neural network.\n",
    "\n",
    "    Feature Engineering: Additional features or better preprocessing might help the neural network model differentiate itself from simple baselines. For instance, consider including features that capture temporal trends, video categories, or other metadata.\n",
    "\n",
    "    Model Type: Given the results, it might be worth exploring other machine learning models, like tree-based models (Random Forest, Gradient Boosting) or linear regression models with regularization, which might provide better interpretability and performance.\n",
    "\n",
    "    Domain Knowledge: It's crucial to understand the context. If a difference of a few thousand views is significant in the domain of YouTube analytics (given monetization or advertising concerns), then even a slight improvement over the median baseline can be valuable.\n",
    "\n",
    "In summary, while the neural network model shows some promise, its performance is close to a simple median predictor. It might be worth exploring other modeling strategies, features, or data sources to enhance the model's predictive power.\n",
    "\n",
    "Sep 12 2023:\n",
    "Data 189627 videos\n",
    "Observations:\n",
    "\n",
    "    The Mean Absolute Error (MAE) of the model on the log-transformed view counts is 1.0339.\n",
    "    When predictions are transformed back to the original scale, the MAE is 483,753.2263.\n",
    "    This is a significant improvement compared to the Mean Baseline MAE of 795,839.5542 and the Median Baseline MAE of 525,261.4406.\n",
    "\n",
    "In essence, the neural network model you've built and trained seems to perform significantly better than naive baselines (mean and median). It's a good practice to compare model performance to such baselines to understand the model's relative performance.\n",
    "\n",
    "Recommendations:\n",
    "\n",
    "    While the model is outperforming the naive baselines, there's still room for improvement. You might consider trying different architectures, other types of embeddings, or even ensemble methods.\n",
    "    If you have more metadata or can extract more features from the video titles or tags, they might help improve the model's performance.\n",
    "    Feature importance analysis can provide insights into which features are most impactful in predicting view counts.\n",
    "    A more comprehensive evaluation could involve other metrics like RMSE (Root Mean Squared Error) or R-squared, which can give a sense of the model's predictive accuracy and explanatory power, respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10811823",
   "metadata": {},
   "source": [
    "The Feed-Forward Neural Network (also known as Dense Neural Network) you've used has already shown a good performance, especially when compared to the baseline models. However, there are a few strategies you can try to potentially improve its performance:\n",
    "\n",
    "    Feature Scaling: Neural networks tend to perform better when the input features are scaled. You can use MinMax scaling or Standard scaling on your input features. Remember to fit the scaler only on the training data and then transform both training and test data.\n",
    "\n",
    "    Architecture Tweaks:\n",
    "        More Layers: You can try adding more layers to the model to see if it captures more intricate patterns.\n",
    "        More Neurons: Increase the number of neurons in each layer. However, more neurons mean more parameters, which can lead to overfitting.\n",
    "        Different Activation Functions: While relu is commonly used, you can also try variants like leaky relu or elu.\n",
    "\n",
    "    Regularization:\n",
    "        L1/L2 Regularization: You can add L1 or L2 regularization to the layers to prevent overfitting.\n",
    "        Increase Dropout: You already have dropout, but you can experiment with different dropout rates.\n",
    "\n",
    "    Learning Rate Schedule: Instead of reducing the learning rate on a plateau, you can also try a time-based decay or step decay.\n",
    "\n",
    "    Optimizers: While Adam is a great general-purpose optimizer, you can also try others like RMSprop or Nadam.\n",
    "\n",
    "    Batch Size: Experiment with different batch sizes. A smaller batch size might offer better generalization at the cost of training time.\n",
    "\n",
    "    Data Augmentation: While data augmentation is more popular in image processing, there are ways to augment tabular data too. Techniques like SMOTE (for imbalanced datasets), noise addition, or even generating synthetic samples can be explored.\n",
    "\n",
    "    Feature Engineering: Consider revisiting the feature engineering phase. Maybe there are some other features or interactions between features that the model might find useful.\n",
    "\n",
    "    Ensemble Methods: Once you have a good model, you can train multiple models and average their predictions. Techniques like bagging can be useful.\n",
    "\n",
    "    Custom Loss Function: Instead of using mean absolute error, you can define custom loss functions that might be better suited for your specific problem.\n",
    "\n",
    "Remember, while trying these suggestions, it's essential to keep an eye on both training and validation performance to ensure the model is not overfitting. Always hold out a portion of your data as a validation set (or use cross-validation) to get an unbiased estimate of the model's performance during the tuning process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "5c99662a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the scaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit on the training data and transform both training and test data\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea99a54e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the fitted scaler to a file\n",
    "joblib.dump(scaler, 'models/my_model_yt_features_FFNN_test_fitted_scaler.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af499935",
   "metadata": {},
   "source": [
    "## Prediction from one saved model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "cf4590dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_views(title, tags, days_since_published, duration_seconds, publication_month, day_of_week, subscriberCount):\n",
    "    # Convert the title and tags to vectors\n",
    "    content_vector = qgr.convertToVector(title, sbert_model, 384)\n",
    "    tags_vector = tags_to_vector(tags, sbert_model, 384)\n",
    "\n",
    "    # Concatenate all the features\n",
    "    features = np.concatenate([content_vector, tags_vector, \n",
    "                               [days_since_published, duration_seconds, publication_month, day_of_week, subscriberCount]])\n",
    "\n",
    "    # Reshape the features to have a shape of (1, -1) because we're predicting for one sample\n",
    "    features = features.reshape(1, -1)\n",
    "\n",
    "    # Scale the features using the scaler\n",
    "    scaled_features = loaded_scaler.transform(features)\n",
    "\n",
    "    # Make the prediction\n",
    "    predicted_log_views = loaded_model.predict(scaled_features)\n",
    "\n",
    "    # Convert the prediction back from log scale\n",
    "    predicted_views = np.expm1(predicted_log_views).flatten()[0]\n",
    "\n",
    "    return predicted_views\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "75412c26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 106ms/step\n",
      "Predicted views: 27839.50390625\n"
     ]
    }
   ],
   "source": [
    "# Load the scaler from the saved file\n",
    "loaded_scaler = joblib.load('models/my_model_yt_features_FFNN_211727traindata_fitted_scaler.pkl')\n",
    "\n",
    "# Load the saved Keras model\n",
    "loaded_model = load_model('models/my_model_yt_features_FFNN_211727traindata.h5')\n",
    "\n",
    "\n",
    "title = \"Are We Living in a Simulation?\"\n",
    "tags = [\"science\"] \n",
    "days_since_published = 10\n",
    "duration_seconds = 5400\n",
    "publication_month = 9\n",
    "day_of_week = 7\n",
    "subscriberCount = 240000\n",
    "\n",
    "predicted_views = predict_views(title, tags, days_since_published, duration_seconds, publication_month, day_of_week, subscriberCount)\n",
    "print(f\"Predicted views: {predicted_views}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "dff42a6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 13ms/step\n",
      "Predicted views: 12695063.0\n"
     ]
    }
   ],
   "source": [
    "title = \"These Pools Help Support Half The People On Earth\"\n",
    "tags = [\"science\",\"tech\"] \n",
    "days_since_published = 30 * 30\n",
    "duration_seconds = 19.23 * 60\n",
    "publication_month = 1\n",
    "day_of_week = 1\n",
    "subscriberCount = 14200000\n",
    "\n",
    "predicted_views = predict_views(title, tags, days_since_published, duration_seconds, publication_month, day_of_week, subscriberCount)\n",
    "print(f\"Predicted views: {predicted_views}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
