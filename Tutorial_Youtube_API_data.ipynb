{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2ef76dad",
   "metadata": {},
   "source": [
    "# YouTube Data Collection with the youtube_data_processing Package"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b5f761a",
   "metadata": {},
   "source": [
    "-------------------------------------------------------------\n",
    "\n",
    "Notebook created by Marcelo, David and GPT, Aug 2023.\n",
    "\n",
    "-------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab71a5b7",
   "metadata": {},
   "source": [
    "Welcome to this tutorial on YouTube data collection! In this notebook, we will explore how to use the youtube_data_processing module, a custom Python module designed to interact with the YouTube Data API, fetch data from it, and organize the data for further analysis.\n",
    "\n",
    "The youtube_data_processing module includes functionalities to fetch statistics about YouTube channels, retrieve a list of videos uploaded by a given channel, and gather detailed information about each video. It also provides utility functions to save the fetched data into JSON files and read data from them. This efficient data collection and storage approach makes it easier to manage large amounts of data and control API quota usage.\n",
    "\n",
    "The module is designed with simplicity and efficiency in mind, allowing you to gather YouTube data with just a few lines of Python code. Whether you are a YouTube channel owner looking to gain insights into your channel's performance, a data scientist wanting to analyze YouTube trends, or a student learning data science and looking for interesting datasets to work with, the youtube_data_processing module can be a great tool to streamline your data collection process.\n",
    "\n",
    "In this tutorial, we will cover:\n",
    "\n",
    "    Setting up the YouTube Data API and getting your API key.\n",
    "    Fetching data from YouTube using the youtube_data_processing package.\n",
    "    Understanding the structure of the fetched data.\n",
    "    Creating flat tables from the fetched data for easier analysis.\n",
    "    Saving and loading data using JSON files.\n",
    "    Plus some additional functions\n",
    "\n",
    "Let's get started and dive into the exciting world of YouTube data!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46e473a3",
   "metadata": {},
   "source": [
    "### Preliminary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5171edd2",
   "metadata": {},
   "source": [
    "First, we need to import the youtube_data_processing module that we'll be using in this tutorial. We can do this with the following line of code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "27722342",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6406bb38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'packages.youtube_data_processing' from '/home/mamaral/Documents/qgr/codes/python/notebooks/computational_essays/packages/youtube_data_processing.py'>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# In case you need to work updte the modeule. This imports the importlib module, \n",
    "# which contains functions that help you control \n",
    "# the runtime process of Python scripts, especially those related to importing and reloading modules.\n",
    "import importlib\n",
    "\n",
    "# Then imports the youtube_data_processing module under the alias yt.\n",
    "from packages import youtube_data_processing as yt\n",
    "\n",
    "# This reloads the youtube_data module. The purpose of this is to ensure that the \n",
    "# latest version of the module is in use, especially if the module has been modified \n",
    "# since the start of the Python session.\n",
    "importlib.reload(yt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2022c08",
   "metadata": {},
   "source": [
    "The youtube_data_processing module interacts with the YouTube Data API, and thus requires an API key. If you don't already have an API key, you'll need to set one up. Here's how you can do that:\n",
    "\n",
    "    Log in to your YouTube account.\n",
    "    Navigate to the Google Developers Console (https://console.developers.google.com/).\n",
    "    Create a new project in the console.\n",
    "    In your new project dashboard, click \"Explore & Enable APIs\".\n",
    "    In the library, find the \"YouTube Data API v3\" under \"YouTube APIs\" and enable it.\n",
    "    After enabling the API, you'll need to create a credential for it. This is what will give you your API key.\n",
    "\n",
    "Once you've done all this, you'll be presented with an API key that you can use in this tutorial. Remember to keep this key safe and don't share it publicly. For more detailed information about getting started with the YouTube Data API, you can check out Google's Getting Started guide (https://developers.google.com/youtube/v3/getting-started)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c09ad6fc",
   "metadata": {},
   "source": [
    "### Overview of the functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7495bfe6",
   "metadata": {},
   "source": [
    "This script essentially collects data from YouTube using the YouTube Data API, processes the data, and organizes it into two dataframes: one for channel data and another for video data. The data collected includes various attributes of channels and videos, such as channel description, video count, view count, subscriber count, video title, publish date, like count, comment count, and many more.\n",
    "\n",
    "Here's a summary of what each function does:\n",
    "\n",
    "    get_channel_statistics(api_key, channel_id): Fetches the statistics for a given YouTube channel using the YouTube Data API. The statistics include the number of subscribers, total views, and total videos, among other data.\n",
    "\n",
    "    write_channel_video_list_to_file(channel_id, video_ids): Writes a list of video IDs to a JSON file. Each video ID is associated with a flag indicating whether the video's data has been fetched from the API.\n",
    "\n",
    "    get_channel_videos_list(channel_id, api_key): Retrieves a list of video IDs for the videos uploaded by a given YouTube channel. This is done using the YouTube Data API's search functionality, which returns a list of videos in reverse chronological order (i.e., newest videos first).\n",
    "\n",
    "    get_channel_video(api_key, video_id): Retrieves detailed data for a single video using the YouTube Data API. The data includes the video's title, description, duration, view count, like count, comment count, and more.\n",
    "\n",
    "    process_channels(api_key, max_channels, api_data_file, channels_file_json): Processes a list of YouTube channels, fetching their statistics and videos using the YouTube Data API. The data is written to a JSON file. The function also manages a quota limit by keeping track of the number of channels processed and stopping when a maximum limit is reached.\n",
    "\n",
    "    create_flat_tables_youtube_data(df_api, df_trend, df_category): Creates two flat tables for the YouTube data. The function uses the API data, trending data, and category data to create two tables: one for the channel data and another for the video data. The function also updates the tables with information on whether a channel has a trending video and the number of trending videos a channel has. The tables are then written to .xlsx files.\n",
    "\n",
    "    fetch_youtube_data(api_key, api_data_file, channels_file_json, maxChannels): This function orchestrates the entire process of fetching data from the YouTube API. It first checks whether there are pre-existing data files; if there are, it loads them; if not, it creates new ones. The function then iterates through a list of channel IDs. For each channel, it checks whether the channel's data has already been fetched. If not, it fetches the data and updates the data files. The function does the same for each video of the channel.\n",
    "\n",
    "The script also handles two types of JSON data:\n",
    "\n",
    "    A list of channels to process, where each channel has a channel ID, a flag indicating whether its statistics have been fetched (fetched_statistics), and a flag indicating whether its videos have been fetched (fetched_videos).\n",
    "    A list of video IDs for each channel, where each video ID is associated with a flag indicating whether the video's data has been fetched (fetched_video)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edc6c992",
   "metadata": {},
   "source": [
    "### Fetching Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32dd859e",
   "metadata": {},
   "source": [
    "In this section, we will be using the fetch_youtube_data function to retrieve data from the YouTube Data API. The function takes four parameters:\n",
    "\n",
    "    api_key: This is your YouTube API key, which authenticates you to the API and allows you to make data requests.\n",
    "\n",
    "    api_data_file: This is the JSON file where the fetched data will be stored. The file will consist of a dictionary, where each key-value pair corresponds to a channelId and its associated data. The dictionary's 'statistics' field contains the statistics for the channel itself, such as the number of subscribers, total views, and total videos, among other data. The 'videos' field contains a list of videos uploaded by the channel. Each entry in this list is a dictionary containing data about a single video, including the video's title, description, view count, like count, comment count, and other details.\n",
    "\n",
    "    channels_file_json: This is the JSON file containing a list of channels to process. Each channel in the list has a channel ID, a flag indicating whether its statistics have been fetched (fetched_statistics), and a flag indicating whether its videos have been fetched (fetched_videos).\n",
    "\n",
    "    maxChannels: This is the maximum number of channels to process. The function will stop processing once it has processed the specified number of channels.\n",
    "\n",
    "In the code snippet below, we're setting the api_key to your actual YouTube API key, the api_data_file to 'youtube_selected_data_API_US.json', the channels_file_json to 'our_YT_channel_ids.json', and maxChannels to 3. We then call the fetch_youtube_data function to begin the data retrieval process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a485b6a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "# api_data_file should have dictionary where each key-value pair corresponds to a channelId and its associated data.\n",
    "#     statistics: This field contains the statistics for the channel itself, \n",
    "# likely including things like the number of subscribers, the total number of views, and the total number of videos, \n",
    "# among other data.  {'yt_api': {'videos': [], 'statistics':...\n",
    "#    videos: This field contains a list of videos uploaded by the channel. \n",
    "# Each entry in this list is a dictionary containing data about a single video, \n",
    "# which might include the video's title, description, view count, like count, comment count, and other details.\n",
    "api_data_file = \"data/YT_data_sample.json\"\n",
    "\n",
    "# channels_file_json has the structure: \n",
    "# [{\"channelID\": \"UCdWIQh9DGG6uhJk8eyIFl1w\", \"fetched_videos\": true, \"fetched_statistics\": true},{...}..]\n",
    "channels_file_json = \"data/YT_channel_ids.json\"\n",
    "\n",
    "# Your API key\n",
    "api_key = \"your_api_key\" \n",
    "\n",
    "# Set a maximum number of channels to process\n",
    "maxChannels = 1\n",
    "\n",
    "# retrieve data from the YouTube Data API\n",
    "yt.fetch_youtube_data(api_key, api_data_file, channels_file_json, maxChannels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "31dbb548",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "key1\n",
      "No videos found for channel UCd_hVSCsmp9viJpr41765dA\n",
      "API Key used up for the day: key1\n",
      "Error details: 'NoneType' object is not iterable\n",
      "key2\n",
      "No videos found for channel UCd_hVSCsmp9viJpr41765dA\n",
      "API Key used up for the day: key2\n",
      "Error details: 'NoneType' object is not iterable\n",
      "data/youtube_data_API_US_1_41.json\n"
     ]
    }
   ],
   "source": [
    "# Updates:\n",
    "# - Loop thru a set of api keys\n",
    "# - Automatically name file\n",
    "\n",
    "#channel_file_json = \"yt_trending_channelid_selection_ranked_updated.json\"\n",
    "channel_file_json = \"data/YT_channel_ids.json\"\n",
    "maxChannels = 100\n",
    "api_keys = ['key1', 'key2']\n",
    "\n",
    "base_number = 1 \n",
    "\n",
    "for api in api_keys:\n",
    "    api_video_file = \"data/youtube_data_API_US_\" + str(base_number) + \"_\" + str(base_number+40) + \".json\"\n",
    "    #print(api_video_file)\n",
    "    try:\n",
    "        print(api)\n",
    "        yt.fetch_youtube_data(api, api_video_file, channel_file_json, maxChannels, True)\n",
    "    except Exception as e: # Catch the specific exception and print it\n",
    "        print(f\"API Key used up for the day: {api}\")\n",
    "        print(f\"Error details: {str(e)}\")\n",
    "print(api_video_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b34c3bd",
   "metadata": {},
   "source": [
    "### Creating flat tables from the fetched data for easier analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "440d73b6",
   "metadata": {},
   "source": [
    "In this section, we leverage the data we fetched from the YouTube Data API to create structured tables that will facilitate further analysis. We accomplish this by reading our JSON files into pandas DataFrames and then processing them with our custom function create_flat_tables_youtube_data from the youtube_data_processing package.\n",
    "\n",
    "This function creates two tables: one for channel data and another for video data. It enriches the channel data with information on whether a channel has a trending video and the number of trending videos a channel has. The resulting tables are saved as .xlsx files, providing a structured and convenient format for any subsequent analysis. This section illustrates the transformation of raw, unstructured data into organized, structured data that is ready for detailed examination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1534f639",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty video data encountered.\n",
      "Empty video data encountered.\n",
      "Empty video data encountered.\n",
      "Empty video data encountered.\n"
     ]
    }
   ],
   "source": [
    "# This line uses the pandas function read_json to read a JSON file and convert it \n",
    "# into a pandas DataFrame. The file 'youtube_selected_data_API_US_54.json' presumably \n",
    "# contains data fetched from the YouTube Data API. The 'lines=True' argument is \n",
    "# used because the file is in a JSON Lines format, where each line is a valid JSON object.\n",
    "# youtube_selected_data_API_US_54.json should have dictionary where each key-value pair corresponds to a channelId \n",
    "# and its associated data. {'yt_api': {'videos': [], 'statistics':...\n",
    "#     statistics: This field contains the statistics for the channel itself, \n",
    "# likely including things like the number of subscribers, the total number of views, and the total number of videos, \n",
    "# among other data.\n",
    "#    videos: This field contains a list of videos uploaded by the channel. \n",
    "\n",
    "df_api = pd.read_json('data/YT_data_sample.json', lines=True)\n",
    "\n",
    "\n",
    "# This line is similar to the previous one, but it reads the file \n",
    "# 'youtube_300_selected_data_trend_US.json', which presumably contains data on \n",
    "# trending videos fetched from a trending database.\n",
    "# the structure is videoID  \\\n",
    "# 0  {'US_trends': [{'row_id': 1424, 'video_id': ....' ...\n",
    "df_trend = pd.read_json('data/sampled_youtube_data_trend.json', lines=True)\n",
    "\n",
    "# This block opens the file 'US_category_id.json' and loads the JSON data into the \n",
    "# variable df_category. This file presumably contains data on the different video \n",
    "# categories on YouTube.\n",
    "with open('data/US_category_id.json') as f:\n",
    "    df_category = json.load(f)\n",
    "\n",
    "\n",
    "xlsx_channel_name = 'data/YT_data_sample_channel.xlsx'\n",
    "xlsx_channel_video = 'data/YT_data_sample_video.xlsx'\n",
    "\n",
    "# Finally, this line calls the create_flat_tables_youtube_data function from the \n",
    "# youtube_data module, passing the three dataframes as arguments. This function \n",
    "# creates two flat tables for the YouTube data (one for the channel data and one \n",
    "# for the video data) and writes them to .xlsx files.\n",
    "yt.create_flat_tables_youtube_data(df_api, df_trend, df_category, xlsx_channel_name, xlsx_channel_video)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94250d48",
   "metadata": {},
   "source": [
    "In this section, we load the structured data from the .xlsx files we created in the previous step. The data is read into pandas DataFrames, which are convenient structures for handling and analyzing tabular data in Python.\n",
    "\n",
    "We have two DataFrames: channel_table and video_table, corresponding to the channel and video data, respectively. By using the head() function, we can display the first few rows of each DataFrame. This helps provide an initial overview of the data structure and the information contained within.\n",
    "\n",
    "This step is crucial in familiarizing ourselves with the data before proceeding with any further analysis or processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "86ef45fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['channelId', 'channelTitle', 'description', 'publishedAt', 'videoCount', 'viewCount', 'subscriberCount', 'country', 'customUrl', 'topicCategories', 'madeForKids', 'keywords', 'hasVideoTrending', 'numberVideoTrending']\n",
      "['videoId', 'title', 'description', 'publishedAt', 'channelId', 'categoryId', 'category', 'viewCount', 'likeCount', 'commentCount', 'tags', 'channelTitle', 'thumbnails', 'isTrending', 'duration', 'dimension', 'definition', 'caption', 'licensedContent', 'projection', 'uploadStatus', 'privacyStatus', 'license', 'embeddable', 'publicStatsViewable', 'madeForKids', 'favoriteCount', 'topicCategories']\n",
      "                  channelId              channelTitle  \\\n",
      "0  UCUyk0KLo7JPLCCh4oRNLzsQ  Quantum Gravity Research   \n",
      "\n",
      "                                         description           publishedAt  \\\n",
      "0  Quantum Gravity Research is a Los Angeles base...  2015-11-17T00:34:43Z   \n",
      "\n",
      "   videoCount  viewCount  subscriberCount country                customUrl  \\\n",
      "0         186   11257198           227000      US  @quantumgravityresearch   \n",
      "\n",
      "                               topicCategories  madeForKids  \\\n",
      "0  ['https://en.wikipedia.org/wiki/Knowledge']            0   \n",
      "\n",
      "                                            keywords  hasVideoTrending  \\\n",
      "0  matrix \"klee irwin\" quantum \"quantum gravity\" ...             False   \n",
      "\n",
      "   numberVideoTrending  \n",
      "0                    0  \n",
      "       videoId                                              title  \\\n",
      "0  EzwQOdg8Tl4     Exploring The Unification of Spirit and Matter   \n",
      "1  cE8J0g6l6xE              Is Consciousness Connected Over Time?   \n",
      "2  jqxYa8fktEc  Exploring The Self-Simulation Hypothesis & Nat...   \n",
      "3  2UiYlwHS8LI  Trailer for Klee Irwin's \"Are We In A Simulati...   \n",
      "4  tY7bkxxwhqE  كلي إيروين - هل نحن في محاكاة؟ - الجزء 4 - فرض...   \n",
      "\n",
      "                                         description           publishedAt  \\\n",
      "0  Do you think the world is described by materia...  2023-07-24T18:58:59Z   \n",
      "1  Check out Klee Irwin's interview on the Know T...  2023-07-17T23:17:10Z   \n",
      "2  WATCH HERE - https://youtu.be/h_6pYpM0Jg8\\n\\nL...  2023-06-23T23:55:45Z   \n",
      "3  Watch the full series here: https://www.youtub...  2023-05-17T16:00:00Z   \n",
      "4  من الواقعي تماما للبشرية أن تغرق في أحد سيناري...  2023-05-08T18:03:24Z   \n",
      "\n",
      "                  channelId  categoryId              category  viewCount  \\\n",
      "0  UCUyk0KLo7JPLCCh4oRNLzsQ          28  Science & Technology       2326   \n",
      "1  UCUyk0KLo7JPLCCh4oRNLzsQ          28  Science & Technology       3968   \n",
      "2  UCUyk0KLo7JPLCCh4oRNLzsQ          28  Science & Technology       3265   \n",
      "3  UCUyk0KLo7JPLCCh4oRNLzsQ          28  Science & Technology      10679   \n",
      "4  UCUyk0KLo7JPLCCh4oRNLzsQ          28  Science & Technology       1562   \n",
      "\n",
      "   likeCount  commentCount  ... licensedContent   projection uploadStatus  \\\n",
      "0        181             9  ...            True  rectangular    processed   \n",
      "1        260            21  ...            True  rectangular    processed   \n",
      "2        138            11  ...            True  rectangular    processed   \n",
      "3        230            25  ...            True  rectangular    processed   \n",
      "4         24             1  ...           False  rectangular    processed   \n",
      "\n",
      "   privacyStatus  license embeddable publicStatsViewable  madeForKids  \\\n",
      "0         public  youtube       True                True        False   \n",
      "1         public  youtube       True                True        False   \n",
      "2         public  youtube       True                True        False   \n",
      "3         public  youtube       True                True        False   \n",
      "4         public  youtube       True                True        False   \n",
      "\n",
      "   favoriteCount                                    topicCategories  \n",
      "0              0         ['https://en.wikipedia.org/wiki/Religion']  \n",
      "1              0         ['https://en.wikipedia.org/wiki/Religion']  \n",
      "2              0        ['https://en.wikipedia.org/wiki/Knowledge']  \n",
      "3              0  ['https://en.wikipedia.org/wiki/Knowledge', 'h...  \n",
      "4              0        ['https://en.wikipedia.org/wiki/Knowledge']  \n",
      "\n",
      "[5 rows x 28 columns]\n"
     ]
    }
   ],
   "source": [
    "# Load the channel and video data from the xlsx files\n",
    "channel_table = pd.read_excel('data/YT_data_sample_channel.xlsx')\n",
    "video_table = pd.read_excel('data/YT_data_sample_video.xlsx')\n",
    "\n",
    "print(channel_table.columns.tolist())\n",
    "print(video_table.columns.tolist())\n",
    "\n",
    "# Display the first few rows of each dataframe\n",
    "print(channel_table.head())\n",
    "print(video_table.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12cf6ff5",
   "metadata": {},
   "source": [
    "### Eliminate duplicated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fd8d159f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of rows: 1\n",
      "Total number of rows: 1\n",
      "Total number of rows: 182\n",
      "Total number of rows: 181\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load the Excel file into a DataFrame\n",
    "channel_table = pd.read_excel('data/YT_data_sample_channel.xlsx')\n",
    "print(\"Total number of rows:\", channel_table.shape[0])\n",
    "\n",
    "# Remove duplicates based on the 'channelId' column and keep the first occurrence\n",
    "channel_table_cleaned = channel_table.drop_duplicates(subset='channelId', keep='first')\n",
    "print(\"Total number of rows:\", channel_table_cleaned.shape[0])\n",
    "\n",
    "# Save the cleaned DataFrame back to the Excel file\n",
    "#channel_table_cleaned.to_excel('data/YT_data_sample_video_clean.xlsx', index=False)\n",
    "\n",
    "# Load the Excel file into a DataFrame\n",
    "video_table = pd.read_excel('data/YT_data_sample_video.xlsx')\n",
    "print(\"Total number of rows:\", video_table.shape[0])\n",
    "\n",
    "# Remove duplicates based on the 'videoId' column and keep the first occurrence\n",
    "video_table_cleaned = video_table.drop_duplicates(subset='videoId', keep='first')\n",
    "print(\"Total number of rows:\", video_table_cleaned.shape[0])\n",
    "\n",
    "# Save the cleaned DataFrame back to the Excel file\n",
    "#video_table_cleaned.to_excel('data/YT_data_sample_video_clean.xlsx', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2900d455",
   "metadata": {},
   "source": [
    "### Data enrichement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5f6c928",
   "metadata": {},
   "source": [
    "This function enriches YouTube channel and video data with calculated statistics.\n",
    "\n",
    "For each channel, the function calculates the mean and standard deviation of 'viewCount', 'likeCount',\n",
    "'commentCount', and 'favoriteCount'. It then merges this statistical data with the original channel data.\n",
    "\n",
    "For the video data, the function calculates the ratio of each video's 'viewCount', 'likeCount', 'commentCount',\n",
    "and 'favoriteCount' to the average count for the respective channel.\n",
    "\n",
    "The enriched data is saved back into new .xlsx files.\n",
    "\n",
    "Parameters:\n",
    "xlsx_channels (str): The path to the .xlsx file containing the channel data.\n",
    "xlsx_videos (str): The path to the .xlsx file containing the video data.\n",
    "xlsx_channels_enriched (str): The path to the .xlsx file to write the enriched channel data.\n",
    "xlsx_videos_enriched (str): The path to the .xlsx file to write the enriched video data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "113221d6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "xlsx_channels = 'data/YT_data_sample_channel.xlsx'\n",
    "xlsx_videos = 'data/YT_data_sample_video.xlsx'\n",
    "xlsx_channels_enriched = 'data/YT_data_sample_channel_enriched.xlsx'\n",
    "xlsx_videos_enriched = 'data/YT_data_sample_video_enriched.xlsx'\n",
    "\n",
    "yt.enriche_xlsx_youtube_data(xlsx_channels, xlsx_videos, xlsx_channels_enriched, xlsx_videos_enriched)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de7060e8",
   "metadata": {},
   "source": [
    "### Merge tables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94ca1946",
   "metadata": {},
   "source": [
    "We can have different tables, so we can merge them if we want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b3308d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge channel tables\n",
    "#xlsx_file1 = \"data/YT_data_sample_channel_enriched.xlsx\"\n",
    "#xlsx_file2 = \"data/YT_data_sample_channel_enriched2.xlsx\"\n",
    "#xlsx_output_filename = \"data/YT_data_sample_channel_enriched_merged.xlsx\"\n",
    "#yt.merge_xlsx_files(xlsx_file1, xlsx_file2, xlsx_output_filename)\n",
    "\n",
    "# merge videos tables\n",
    "#xlsx_file1 = \"data/YT_data_sample_video_enriched.xlsx\"\n",
    "#xlsx_file2 = \"data/YT_data_sample_video_enriched2.xlsx\"\n",
    "#xlsx_output_filename = \"data/YT_data_sample_video_enriched_merged.xlsx\"\n",
    "#yt.merge_xlsx_files(xlsx_file1, xlsx_file2, xlsx_output_filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0e3f10bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xlsx/channel_table_youtube_patreon123_153_data_enriched.xlsx\n",
      "xlsx/channel_table_youtube_patreon80_123_data_enriched.xlsx\n",
      "xlsx/channel_table_youtube_trend175_200_data_enriched.xlsx\n",
      "xlsx/channel_table_youtube_trend125_150_data_enriched.xlsx\n",
      "xlsx/channel_table_youtube_trend0_25_data_enriched.xlsx\n",
      "xlsx/channel_table_youtube_trend200_230_data_enriched.xlsx\n",
      "xlsx/channel_table_youtube_trend150_175_data_enriched.xlsx\n",
      "xlsx/channel_table_youtube_trend75_100_data_enriched.xlsx\n",
      "xlsx/channel_table_youtube_trend25_50_data_enriched.xlsx\n",
      "xlsx/channel_table_youtube_trend100_125_data_enriched.xlsx\n",
      "xlsx/channel_table_youtube_patreon39_80_data_enriched.xlsx\n",
      "xlsx/channel_table_youtube_patreon1_39_data_enriched.xlsx\n",
      "xlsx/channel_table_youtube_trend50_75_data_enriched.xlsx\n"
     ]
    }
   ],
   "source": [
    "#full folder\n",
    "\n",
    "# Path to the folder containing the channel files\n",
    "channel_files_path = 'data/xlsx/channel_table_*.xlsx'\n",
    "\n",
    "# Read the updated merged data\n",
    "data_ini_path = 'data/xlsx/YT_data_sample_channel_enriched_merged.xlsx'\n",
    "merged_data = pd.read_excel(data_ini_path)\n",
    "\n",
    "# List of channel files\n",
    "channel_files = glob.glob(channel_files_path)\n",
    "\n",
    "# Loop through channel files\n",
    "for channel_file in channel_files:\n",
    "    if channel_file == data_ini_path:\n",
    "        continue\n",
    "    channel_data = pd.read_excel(channel_file)\n",
    "    print(channel_file)\n",
    "    # Concatenate the two dataframes into one\n",
    "    merged_data = pd.concat([merged_data, channel_data], ignore_index=True)\n",
    "\n",
    "# Save the final merged data\n",
    "output_filename = 'YT_data_sample_channel_enriched_merged_full.xlsx'\n",
    "merged_data.to_excel(output_filename, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6dda88d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xlsx/video_table_youtube_trend200_230_data_cleaned_enriched.xlsx\n",
      "xlsx/video_table_youtube_patreon39_80_data_cleaned_enriched.xlsx\n",
      "xlsx/video_table_youtube_patreon1_39_data_cleaned_enriched.xlsx\n",
      "xlsx/video_table_youtube_trend0_25_data_cleaned_enriched.xlsx\n",
      "xlsx/video_table_youtube_trend150_175_data_cleaned_enriched.xlsx\n",
      "xlsx/video_table_youtube_trend175_200_data_cleaned_enriched.xlsx\n",
      "xlsx/video_table_youtube_trend75_100_data_cleaned_enriched.xlsx\n",
      "xlsx/video_table_youtube_trend25_50_data_cleaned_enriched.xlsx\n",
      "xlsx/video_table_youtube_trend50_75_data_cleaned_enriched.xlsx\n",
      "xlsx/video_table_youtube_patreon80_123_data_cleaned_enriched.xlsx\n",
      "xlsx/video_table_youtube_patreon123_153_data_cleaned_enriched.xlsx\n",
      "xlsx/video_table_youtube_trend100_125_data_cleaned_enriched.xlsx\n",
      "xlsx/video_table_youtube_trend125_150_data_cleaned_enriched.xlsx\n"
     ]
    }
   ],
   "source": [
    "# Path to the folder containing the video files\n",
    "video_files_path = 'xlsx/video_table_*.xlsx'\n",
    "\n",
    "# Read the updated merged data\n",
    "data_ini_path = 'xlsx/YT_data_sample_video_enriched_merged.xlsx'\n",
    "merged_data = pd.read_excel(data_ini_path)\n",
    "\n",
    "# List of video files\n",
    "video_files = glob.glob(video_files_path)\n",
    "\n",
    "# Loop through video files\n",
    "for video_file in video_files:\n",
    "    if video_file == data_ini_path:\n",
    "        continue\n",
    "    video_data = pd.read_excel(video_file)\n",
    "    print(video_file)\n",
    "    # Concatenate the two dataframes into one\n",
    "    merged_data = pd.concat([merged_data, video_data], ignore_index=True)\n",
    "\n",
    "# Save the final merged data\n",
    "output_filename = 'YT_data_sample_video_enriched_merged_full.xlsx'\n",
    "merged_data.to_excel(output_filename, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7816da8",
   "metadata": {},
   "source": [
    "### Eliminate/check duplicated videos in .json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "375d5f82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of channels processed: 201\n"
     ]
    }
   ],
   "source": [
    "# Read the JSON data into a DataFrame\n",
    "df = pd.read_json('yt_trending_channelid_selection_ranked.json')\n",
    "\n",
    "# Count the channels where fetched_videos is True\n",
    "processed_channels_count = df[df['fetched_videos'] == True].shape[0]\n",
    "\n",
    "# Print the result\n",
    "print(f\"Number of channels processed: {processed_channels_count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "204510f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of duplicated channels among processed ones: 0\n"
     ]
    }
   ],
   "source": [
    "# Filter the DataFrame to include only the processed channels (fetched_videos = True)\n",
    "processed_channels_df = df[df['fetched_videos'] == True]\n",
    "\n",
    "# Check for duplicates in the 'channelID' column and sum the result to get the count of duplicates\n",
    "duplicated_channels_count = processed_channels_df['channelID'].duplicated().sum()\n",
    "\n",
    "# Print the result\n",
    "print(f\"Number of duplicated channels among processed ones: {duplicated_channels_count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "018306b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the channel IDs from the text file\n",
    "with open('channelsProcessed', 'r') as file:\n",
    "    processed_channels = set(line.strip() for line in file)\n",
    "\n",
    "# Load the JSON data from the file\n",
    "with open('yt_trending_channelid_selection_ranked.json', 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Update the values based on whether the channel ID is in the set\n",
    "for item in data:\n",
    "    channel_id = item['channelID']\n",
    "    item['fetched_videos'] = channel_id in processed_channels\n",
    "    item['fetched_statistics'] = item['fetched_videos']\n",
    "\n",
    "# Sort the data so that processed ones come first\n",
    "data.sort(key=lambda x: x['fetched_videos'], reverse=True)\n",
    "\n",
    "# Save the updated data back to a JSON file\n",
    "with open('yt_trending_channelid_selection_ranked_updated.json', 'w') as file:\n",
    "    json.dump(data, file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "9264e54b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of channels processed: 199\n"
     ]
    }
   ],
   "source": [
    "# Read the JSON data into a DataFrame\n",
    "df = pd.read_json('yt_trending_channelid_selection_ranked_updated.json')\n",
    "\n",
    "# Count the channels where fetched_videos is True\n",
    "processed_channels_count = df[df['fetched_videos'] == True].shape[0]\n",
    "\n",
    "# Print the result\n",
    "print(f\"Number of channels processed: {processed_channels_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e1bd207",
   "metadata": {},
   "source": [
    "### Additional Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "055325d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File saved: yt_channel_aparently_not_processed.json.\n"
     ]
    }
   ],
   "source": [
    "# create a new json file to process with fetch_youtube_data from csv with channel ids\n",
    "channels_id_csv_file = \"yt_channel_aparently_not_processed.csv\"\n",
    "channels_id_json_file = \"yt_channel_aparently_not_processed.json\"\n",
    "yt.channel_ids_csv_to_json(channels_id_csv_file,channels_id_json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b15e2331",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Channel: UCUyk0KLo7JPLCCh4oRNLzsQ, Channel Title: Quantum Gravity Research\n",
      "  Video: What Is Reality? [Official Film], Views: 4244127\n",
      "  Video: Hacking Reality [Official Film], Views: 2052430\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# get the top videos by channel\n",
    "\n",
    "xlsx_videos = 'data/YT_data_sample_video_enriched.xlsx'\n",
    "\n",
    "top_videos = yt.get_top_videos(xlsx_videos, 2)\n",
    "\n",
    "for channelId, group in top_videos.groupby('channelId'):\n",
    "    print(f\"Channel: {channelId}, Channel Title: {group.iloc[0]['channelTitle']}\")\n",
    "    for index, row in group.iterrows():\n",
    "        print(f\"  Video: {row['title']}, Views: {row['viewCount']}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "820509ca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
