{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d835218e",
   "metadata": {},
   "source": [
    "# Scientific Data for Large Language Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95eab95d",
   "metadata": {},
   "source": [
    "-------------------------------------------------------------\n",
    "Notebook created by Marcelo Amaral.\n",
    "Assisted by GPT, a language model developed by OpenAI.\n",
    "-------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fa26ea6",
   "metadata": {},
   "source": [
    "This Jupyter notebook is part of a process that is designed to convert scientific articles in PDF format into a structured data format (JSON), which can be used to train large language models. The process involves two main steps:\n",
    "\n",
    "    PDF to XML Conversion: This step is performed outside of this notebook, using a tool called GROBID. GROBID (or GeneRation Of BIbliographic Data) is a machine learning library that converts PDFs into structured TEI-encoded documents with a particular focus on technical and scientific articles.\n",
    "\n",
    "    XML to JSON Conversion: This is the step carried out in this notebook. We use Python's xml.etree.ElementTree to parse the XML files and extract relevant information such as the title, date, authors, abstract, and the content of the paper in LaTeX format.\n",
    "\n",
    "The output of this process is a collection of JSON files, each corresponding to a scientific paper. Each JSON file contains the paper's title, date, authors, abstract, and the main content formatted in LaTeX. The main content includes the sections of the paper and any equations written in LaTeX.\n",
    "\n",
    "This structured data format is much more convenient for training large language models, as it allows for easy access to the different components of a scientific paper. Furthermore, the use of LaTeX for formatting the main content preserves the structure and presentation of mathematical equations and scientific expressions.\n",
    "\n",
    "Note: This process assumes that the directory structure used for storing PDFs is maintained when storing the corresponding XML and JSON files. This makes it easy to trace back the processed data to the original PDF if required.\n",
    "\n",
    "Please be aware that the quality of the output data depends heavily on the quality of the input PDFs and the effectiveness of the GROBID tool in converting these PDFs into structured XML files. Some documents may not be processed correctly due to inconsistencies in the XML structure, errors in the original PDFs, or limitations of the GROBID tool."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52c4ba06",
   "metadata": {},
   "source": [
    "## Import necessary libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb2c6f5c",
   "metadata": {},
   "source": [
    "from langchain.document_loaders import TextLoader: This line imports the TextLoader class from the langchain.document_loaders module. TextLoader is a custom class that is designed to load documents from a given file path. In the context of this notebook, it is used to load XML files for processing.\n",
    "\n",
    "import os: This line imports the built-in Python os module. This module provides a portable way of using operating system dependent functionality such as reading or writing to the file system, starting or killing processes, and more. In this notebook, it is mainly used for file and directory manipulation.\n",
    "\n",
    "import re: This line imports the built-in Python re module, which stands for Regular Expressions. Regular expressions are a powerful tool for various kinds of string manipulation. They are a domain-specific language (DSL) that is present in various forms in almost all modern programming languages.\n",
    "\n",
    "import json: This line imports the built-in Python json module. This module provides an easy way to encode and decode data in JSON. The JSON format is a popular data interchange format and is used in this notebook to save the extracted information from the scientific papers.\n",
    "\n",
    "The xml.etree.ElementTree module in Python provides a lightweight and efficient API for parsing and creating XML data. It is part of Python's standard library and doesn't require any additional installation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9449ee36",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import TextLoader\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import xml.etree.ElementTree as ET"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f7623ce",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d216520b",
   "metadata": {},
   "source": [
    "The function recursive_text_extraction(element) is used to extract all the text nested within an XML element, including text within child elements.\n",
    "\n",
    "Here is a brief explanation of the function:\n",
    "\n",
    "    The function takes as input an XML element.\n",
    "\n",
    "    It initializes text with the text of the current XML element, or an empty string if the element has no text.\n",
    "\n",
    "    It then iterates over each child of the current XML element.\n",
    "\n",
    "    For each child, it recursively calls recursive_text_extraction(child) to extract the text within the child, including any text within nested child elements. This text is then appended to text.\n",
    "\n",
    "    If the child element has a tail (text that comes after the child element but is still inside the parent element), this is also appended to text.\n",
    "\n",
    "    Finally, the function returns the complete text that has been extracted.\n",
    "\n",
    "In this way, the function is able to extract all the text within an XML element, no matter how deeply nested the text might be within child elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9a455421",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recursive_text_extraction(element):\n",
    "    text = element.text or \"\"\n",
    "    for child in element:\n",
    "        text += recursive_text_extraction(child)\n",
    "        if child.tail:\n",
    "            text += child.tail\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b222955d",
   "metadata": {},
   "source": [
    "The extract_information_from_tei function is designed to parse an XML string that is structured according to the Text Encoding Initiative (TEI) guidelines. The TEI is a standard for the representation of texts in digital form, widely used in the field of digital humanities, libraries, and linguistics. This function extracts key information from the XML file such as the title, authors, abstract, keywords, and content of the document.\n",
    "\n",
    "Here's a brief overview of what the function does:\n",
    "\n",
    "    Parse XML: The function starts by using the ET.fromstring() method from the xml.etree.ElementTree module to parse the XML string into an XML ElementTree object.\n",
    "\n",
    "    Extract Metadata: The function then extracts the title, authors, date, abstract, and keywords from the XML tree. This is done using the find and findall methods to locate the relevant elements in the XML tree.\n",
    "\n",
    "    Extract Content: The function also extracts the body content of the document, including the text of each section and any LaTeX equations in each section. This is done by iterating over the div elements in the body of the document, which represent individual sections.\n",
    "\n",
    "    Convert to LaTeX: The extracted text is then formatted into LaTeX syntax. This is done by wrapping section titles in \\section{...}, wrapping equations in \\begin{equation} ... \\end{equation}, and simply appending the text of each paragraph.\n",
    "\n",
    "    Return as Dictionary: Finally, the function returns a dictionary with the extracted information. The dictionary keys are 'title', 'authors', 'date', 'abstract', 'keywords', and 'latex_doc'. The 'latex_doc' key contains the full LaTeX-formatted text of the document, while the other keys contain the respective pieces of metadata.\n",
    "\n",
    "In short, this function serves as a TEI-XML to LaTeX converter, specifically tailored for scientific documents. It extracts both the metadata and content from the XML file and formats them in a way that's ready for further LaTeX processing or display."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c80a872d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_information_from_tei(tei_string):\n",
    "    # Parse the XML string\n",
    "    root = ET.fromstring(tei_string)\n",
    "\n",
    "    # XML namespaces\n",
    "    ns = {'tei': 'http://www.tei-c.org/ns/1.0'}\n",
    "\n",
    "    # LaTeX document\n",
    "    latex_doc = []\n",
    "    \n",
    "    # Details dictionary\n",
    "    details = {}\n",
    "\n",
    "    # Extract the title - only proceed if we have a title\n",
    "    title = root.find('.//tei:titleStmt/tei:title', ns)\n",
    "    if title is None or title.text is None:\n",
    "        raise ValueError(\"Missing title in document\")\n",
    "        \n",
    "    # Extract the title\n",
    "    title = root.find('.//tei:titleStmt/tei:title', ns)\n",
    "    if title is not None and title.text is not None:\n",
    "        details['title'] = title.text[:1000] # restricting according Milvus  FieldSchema(name=\"title\", dtype=DataType.VARCHAR, max_length=1000), \n",
    "        latex_doc.append('\\\\title{' + title.text + '}')\n",
    "\n",
    "    # Extract the date\n",
    "    date = root.find('.//tei:publicationStmt/tei:date', ns)\n",
    "    if date is not None and date.text is not None:\n",
    "        details['date'] = date.text\n",
    "        latex_doc.append('\\\\date{' + date.text + '}')\n",
    "        \n",
    "    # Extract the authors from the main document only\n",
    "    authors = []\n",
    "    for author in root.findall('.//tei:teiHeader/tei:fileDesc/tei:sourceDesc/tei:biblStruct/tei:analytic/tei:author', ns):\n",
    "        forename = author.find('tei:persName/tei:forename', ns)\n",
    "        surname = author.find('tei:persName/tei:surname', ns)\n",
    "        if forename is not None and surname is not None:\n",
    "            authors.append(forename.text + ' ' + surname.text)\n",
    "            latex_doc.append('\\\\author{' + forename.text + ' ' + surname.text + '}')\n",
    "    details['authors'] = authors\n",
    "\n",
    "    # Extract the abstract\n",
    "    abstract = root.find('.//tei:abstract', ns)\n",
    "    if abstract is not None:\n",
    "        details['abstract'] = ' '.join([p.text for p in abstract.findall('.//tei:p', ns)])\n",
    "        latex_doc.append('\\\\begin{abstract}')\n",
    "        for p in abstract.findall('.//tei:p', ns):\n",
    "            latex_doc.append(p.text)\n",
    "        latex_doc.append('\\\\end{abstract}')\n",
    "\n",
    "    # Extract the keywords\n",
    "    keywords = root.find('.//tei:keywords', ns)\n",
    "    if keywords is not None:\n",
    "        details['keywords'] = [term.text for term in keywords.findall('tei:term', ns)]\n",
    "        latex_doc.append('\\\\keywords{' + ', '.join([term.text for term in keywords.findall('tei:term', ns)]) + '}')\n",
    "\n",
    "    # Process each section in the body of the document\n",
    "    for body in root.findall('.//tei:body', ns):\n",
    "        for div in body.findall('.//tei:div', ns):\n",
    "            # Add section\n",
    "            section_title = div.find('tei:head', ns)\n",
    "            if section_title is not None:\n",
    "                latex_doc.append('\\\\section{' + section_title.text + '}')\n",
    "            \n",
    "            # Process each child element in the div\n",
    "            for child in div:\n",
    "                # If the child is a paragraph\n",
    "                if child.tag == '{http://www.tei-c.org/ns/1.0}p':\n",
    "                    paragraph_text = recursive_text_extraction(child)\n",
    "                    latex_doc.append(paragraph_text)\n",
    "\n",
    "                # If the child is a formula\n",
    "                elif child.tag == '{http://www.tei-c.org/ns/1.0}formula':\n",
    "                    # Add equation\n",
    "                    latex_doc.append('\\\\begin{equation}')\n",
    "                    latex_doc.append(child.text.strip())  # strip leading and trailing whitespace\n",
    "                    latex_doc.append('\\\\end{equation}')\n",
    "\n",
    "                    \n",
    "    # Begin the bibliography\n",
    "    latex_doc.append('\\\\begin{thebibliography}{99}')\n",
    "\n",
    "    # Extract the references\n",
    "    for biblStruct in root.findall('.//tei:div[@type=\"references\"]/tei:listBibl/tei:biblStruct', ns):\n",
    "        # Extract the authors\n",
    "        authors = [forename.text + ' ' + surname.text for forename, surname in zip(biblStruct.findall('.//tei:author/tei:persName/tei:forename', ns), biblStruct.findall('.//tei:author/tei:persName/tei:surname', ns))]\n",
    "\n",
    "        # Extract the title\n",
    "        title = biblStruct.find('.//tei:title', ns)\n",
    "\n",
    "        # Extract the year\n",
    "        year = biblStruct.find('.//tei:date', ns)\n",
    "\n",
    "        # Extract the publisher (for books) or journal title (for articles)\n",
    "        publisher = biblStruct.find('.//tei:publisher', ns)\n",
    "        journal = biblStruct.find('.//tei:title[@level=\"j\"]', ns)\n",
    "\n",
    "        # Extract the volume and page numbers (for articles)\n",
    "        volume = biblStruct.find('.//tei:biblScope[@unit=\"volume\"]', ns)\n",
    "        page = biblStruct.find('.//tei:biblScope[@unit=\"page\"]', ns)\n",
    "\n",
    "        # Format the reference for the bibliography\n",
    "        reference = ', '.join(filter(None, [', '.join(authors), (title.text if title is not None else None), (year.text if year is not None else None), (publisher.text if publisher is not None else None), (journal.text if journal is not None else None), (volume.text if volume is not None else None), (page.text if page is not None else None)]))\n",
    "        latex_doc.append('\\\\bibitem{' + biblStruct.attrib['{http://www.w3.org/XML/1998/namespace}id'] + '} ' + reference + '.')\n",
    "\n",
    "    # End the bibliography\n",
    "    latex_doc.append('\\\\end{thebibliography}')\n",
    "\n",
    "    details['latex_doc'] = '\\n'.join(latex_doc)\n",
    "    return details\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b28fccbd",
   "metadata": {},
   "source": [
    "The slugStrip function is used to clean up a given string, specifically to prepare it for use as a file name or URL slug. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "92b5122f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def slugStrip(instr):\n",
    "    outstr = re.sub('[^a-zA-Z0-9 ]','',instr)\n",
    "    while \"  \" in outstr:\n",
    "        outstr = outstr.replace(\"  \",\" \")\n",
    "    outstr = outstr.replace(\" \",\"-\")\n",
    "    outstr = outstr.lower()\n",
    "    if len(outstr) > 0:\n",
    "        if outstr[0] == \"-\":\n",
    "            outstr = outstr[1:]\n",
    "    if len(outstr) > 0: \n",
    "        if outstr[-1] == \"-\":\n",
    "            outstr = outstr[:-1]\n",
    "    return outstr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82da9a46",
   "metadata": {},
   "source": [
    "The create_file_name function is used to convert a given string (in this case, a title) into a suitable file name for a JSON file. The steps it takes to do this are:\n",
    "\n",
    "    Clean the Title: It uses the slugStrip function to remove any special characters and spaces from the title and converts it to lowercase.\n",
    "\n",
    "    Replace Spaces with Underscores: It then replaces any spaces that remain in the title with underscores using the replace method. This is a common practice when creating file names because spaces can sometimes cause issues in file paths.\n",
    "\n",
    "    Limit Length: To prevent the file name from being too long (which can cause issues with some file systems), it then limits the length of the title to 250 characters. This leaves room for the \".json\" extension while still ensuring that the total length of the file name stays within the common maximum limit of 255 characters.\n",
    "\n",
    "    Add Extension: Finally, it adds the \".json\" extension to the title, completing the file name.\n",
    "\n",
    "The function then returns the resulting file name. This function is especially useful when you want to create file names that reflect the contents of the files, and you are dealing with user-provided or otherwise unpredictable text for the titles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7fb73f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_file_name(title):\n",
    "    # Use the slugStrip function to clean the title\n",
    "    title = slugStrip(title)\n",
    "\n",
    "    # Replace spaces with underscores\n",
    "    title = title.replace(' ', '_')\n",
    "\n",
    "    # Limit length to 250 characters (to allow for the .json extension)\n",
    "    title = title[:250]\n",
    "    \n",
    "    # Append the .json extension\n",
    "    title += '.json'\n",
    "\n",
    "    return title"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7421994d",
   "metadata": {},
   "source": [
    "## Extracting information of one file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83554f18",
   "metadata": {},
   "source": [
    "The code in this section demonstrates how to load a single XML file and extract information from it.\n",
    "\n",
    "Here's a step-by-step breakdown of the process:\n",
    "\n",
    "    Specify the File Location: The location of the XML file is specified using the dir_path and file_name variables. These are then combined using the os.path.join() function to create the full path to the file.\n",
    "\n",
    "    Load the File: The TextLoader class is used to load the XML file from the specified location. The load() method of the TextLoader instance is then called to read the content of the file.\n",
    "\n",
    "    Extract Information: For each document loaded (in this case, there's just one), the extract_information_from_tei() function is called with the document's content as an argument. This function extracts various pieces of information from the document, such as the title, authors, abstract, keywords, and the document's main content, and stores them in a dictionary (latex_doc).\n",
    "\n",
    "At this point, you could do whatever you like with the latex_doc dictionary. You might print it out to see the extracted information, or write it to a file for later use. For example, you could convert it to JSON and write it to a .json file, or use it to generate a LaTeX document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "204eb071",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the file with TextLoader\n",
    "dir_path = \"data/xml/\"\n",
    "file_name = \"arxiv QSF 2023.tei.xml\"\n",
    "\n",
    "full_path = os.path.join(dir_path, file_name)\n",
    "\n",
    "loader = TextLoader(full_path)\n",
    "documents = loader.load()\n",
    "\n",
    "# Extract the information from each loaded document\n",
    "for document in documents:\n",
    "    latex_doc = extract_information_from_tei(document.page_content)\n",
    "    # do something with latex_doc, for example print it or write it to a file\n",
    "\n",
    "len(latex_doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70dd7bee",
   "metadata": {},
   "source": [
    "In this section, we demonstrate how to save the extracted data (stored in the latex_doc dictionary) to a JSON file, and then how to load it back into Python.\n",
    "\n",
    "Here's a step-by-step explanation:\n",
    "\n",
    "    Create File Name: The create_file_name() function is called with the title of the document as an argument. This function cleans the title by removing special characters and spaces, replacing them with underscores, and appending the '.json' extension. The output of this function is the name of the JSON file where the document's data will be saved.\n",
    "\n",
    "    Specify Full Path: The os.path.join() function is used to concatenate the directory path (dir_path) and the JSON file name (file_name) to create the full path to the JSON file.\n",
    "\n",
    "    Save Data to JSON File: A JSON file is opened in write mode with the open() function. The json.dump() function is then used to write the latex_doc dictionary to the JSON file. The file is automatically closed at the end of the with block.\n",
    "\n",
    "    Load Data from JSON File: The same JSON file is opened in read mode. The json.load() function is used to load the data from the JSON file into a new latex_doc dictionary.\n",
    "\n",
    "After loading the data, you can access any of the information in the dictionary using its keys. In this case, the title of the document is accessed with latex_doc['title'], and the main content of the document (in LaTeX format) is accessed with latex_doc['latex_doc']. These are then printed to the console."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5170a222",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QUASICRYSTALLINE SPIN FOAM WITH MATTER: DEFINITIONS AND EXAMPLES\n"
     ]
    }
   ],
   "source": [
    "# testing saving a json file and loading\n",
    "file_name = create_file_name( latex_doc['title'])\n",
    "full_path = os.path.join(dir_path, file_name)\n",
    "\n",
    "\n",
    "# Save the latex_doc dictionary to a JSON file\n",
    "with open(full_path, 'w') as f:\n",
    "    json.dump(latex_doc, f)\n",
    "    \n",
    "# Load the data from the JSON file\n",
    "with open(full_path, 'r') as f:\n",
    "    latex_doc = json.load(f)\n",
    "\n",
    "# Now you can access the information\n",
    "print(latex_doc['title'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "498c8004",
   "metadata": {},
   "source": [
    "## Geting the information for a full directory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec8e4d12",
   "metadata": {},
   "source": [
    "This section of the code automates the process of extracting data from multiple .tei.xml files located in a directory structure, and saving the extracted data to corresponding .json files in a parallel directory structure.\n",
    "\n",
    "Here's a step-by-step explanation:\n",
    "\n",
    "    Walk Through Directory Structure: The os.walk() function is used to traverse through the directory structure of the source directory (dir_source_path). For each directory it encounters, it returns the directory path, the names of any subdirectories, and the names of any files.\n",
    "\n",
    "    Process Each .tei.xml File: For each file in the current directory, if the file name ends with .tei.xml, the code processes that file. It creates the full source file path by joining the directory path and the file name.\n",
    "\n",
    "    Load Document and Extract Information: The TextLoader class is used to load the document from the .tei.xml file. The extract_information_from_tei() function is then used to extract data from the document. This function returns a dictionary, latex_doc, containing the extracted data. If there's an error during the extraction process (for example, if the document doesn't have a title), the code prints an error message and skips to the next document.\n",
    "\n",
    "    Create Destination Directory Path: The relative path of the current directory is computed with respect to the source directory path. The destination directory path is then created by joining the destination directory path (dir_destination_path) and the relative directory path. If the destination directory doesn't already exist, it's created with os.makedirs().\n",
    "\n",
    "    Create Destination File Name: The create_file_name() function is called with the title of the document as an argument to create the name of the .json file where the document's data will be saved.\n",
    "\n",
    "    Create Full Destination File Path: The full destination file path is created by joining the destination directory path and the .json file name.\n",
    "\n",
    "    Save Data to JSON File: Finally, the latex_doc dictionary is saved to the .json file using the json.dump() function.\n",
    "\n",
    "By running this code, you can automate the process of extracting data from a large number of .tei.xml files and saving that data to .json files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "adf8ae59",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_source_path = \"data/xml/papers_xml/\"\n",
    "dir_destination_path = \"data/xml/papers_json/\"\n",
    "\n",
    "# Walk through the source directory structure\n",
    "for dirpath, dirnames, filenames in os.walk(dir_source_path):\n",
    "    # Process each .tei.xml file in the current directory\n",
    "    for filename in filenames:\n",
    "        if filename.endswith('.tei.xml'):\n",
    "            # Create the full source file path\n",
    "            full_source_path = os.path.join(dirpath, filename)\n",
    "\n",
    "            loader = TextLoader(full_source_path)\n",
    "            documents = loader.load()\n",
    "\n",
    "            # Extract the information from each loaded document\n",
    "            for document in documents:\n",
    "                try:\n",
    "                    latex_doc = extract_information_from_tei(document.page_content)\n",
    "                except ValueError as e:\n",
    "                    print(f\"Skipping file due to error: {e}\")\n",
    "                    continue\n",
    "        \n",
    "                latex_doc = extract_information_from_tei(document.page_content)\n",
    "\n",
    "                # Create the destination directory path\n",
    "                relative_dirpath = os.path.relpath(dirpath, dir_source_path)\n",
    "                full_destination_dirpath = os.path.join(dir_destination_path, relative_dirpath)\n",
    "\n",
    "                # Create the destination directory if it does not exist\n",
    "                os.makedirs(full_destination_dirpath, exist_ok=True)\n",
    "\n",
    "                # Create the destination file name\n",
    "                file_destination_name = create_file_name(latex_doc['title'])\n",
    "\n",
    "                # Create the full destination file path\n",
    "                full_destination_path = os.path.join(full_destination_dirpath, file_destination_name)\n",
    "\n",
    "                # Save the latex_doc dictionary to a JSON file\n",
    "                with open(full_destination_path, 'w') as f:\n",
    "                    json.dump(latex_doc, f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58aa6e60",
   "metadata": {},
   "source": [
    "## Final Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "907c49e6",
   "metadata": {},
   "source": [
    "With the conclusion of this process, we have successfully transformed the raw .tei.xml files, that were initially extracted from scientific articles, into structured JSON files. This structured format is more accessible and manageable, enabling the use of standard Python tools for data manipulation and analysis in a straightforward manner.\n",
    "\n",
    "These JSON files contain valuable information such as the title, authors, publication date, abstract, keywords, and the content of the article in LaTeX format. This provides a rich dataset for various natural language processing (NLP) tasks.\n",
    "\n",
    "For instance, we can load these JSON files into pandas DataFrames to carry out exploratory data analysis. In addition, it is possible to preprocess the text data by performing operations like tokenization, stemming or lemmatization, and removal of stop words.\n",
    "\n",
    "Once the data is preprocessed, we can convert the text into numerical features using a variety of techniques, including Bag-of-Words, TF-IDF, or word embeddings. These numerical features can then be used as input for machine learning models. Depending on the task at hand, these models could be used for text classification, information retrieval, topic modelling, text generation, or even more complex tasks like machine translation or question answering.\n",
    "\n",
    "Furthermore, the structured format of the JSON files allows for easy metadata analysis. For example, one could analyze trends in authorship or study the usage and popularity of certain keywords over time.\n",
    "\n",
    "It's important to note that the quality of data fed into the machine learning models significantly influences the quality of the output. Therefore, it's critical to ensure that the data is as clean and well-structured as possible. The methods and techniques outlined in this notebook are aimed at facilitating this process.\n",
    "\n",
    "This notebook serves as a pre-processing step, preparing the ground for more sophisticated NLP and machine learning workflows. With the structured data in hand, we are now ready to delve deeper into the world of data science and machine learning, leveraging the information contained in these scientific articles to extract valuable insights or make predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06347e3b",
   "metadata": {},
   "source": [
    "## Appendix: Using GROBID to Extract Information from Nested Directories with Scientific PDF Files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f181951f",
   "metadata": {},
   "source": [
    "In this appendix, we will provide step-by-step instructions on how to use GROBID, a machine learning library for extracting structured data from scientific literature, including PDF files. We will explain how to install and run GROBID, how to use the GROBID Python client, and how to process PDF files in nested directories. We will also demonstrate how to replicate the directory structure from one location to another."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0873efc5",
   "metadata": {},
   "source": [
    "## Download and Install GROBID"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00a6cd76",
   "metadata": {},
   "source": [
    "Download the GROBID source code:\n",
    "\n",
    "```\n",
    "wget https://github.com/kermitt2/grobid/archive/0.7.3.zip\n",
    "```\n",
    "\n",
    "Unzip the downloaded file:\n",
    "```\n",
    "unzip 0.7.3.zip\n",
    "```\n",
    "\n",
    "Navigate to the GROBID directory:\n",
    "```\n",
    "cd grobid-0.7.3/\n",
    "```\n",
    "\n",
    "Build and install GROBID:\n",
    "```\n",
    "./gradlew clean install\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bed73cdd",
   "metadata": {},
   "source": [
    "## Configure GROBID"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8480c97e",
   "metadata": {},
   "source": [
    "You can customize the configuration of GROBID to suit your needs:\n",
    "\n",
    "```\n",
    "nano grobid-home/config/grobid.yaml\n",
    "```\n",
    "\n",
    "After configuring, run GROBID:\n",
    "```\n",
    "./gradlew run\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "758bdd3c",
   "metadata": {},
   "source": [
    "## Install the GROBID Python Client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42f2646e",
   "metadata": {},
   "source": [
    "Navigate back to the parent directory:\n",
    "```\n",
    "cd .. \n",
    "```\n",
    "\n",
    "Clone the GROBID Python client repository:\n",
    "```\n",
    "git clone https://github.com/kermitt2/grobid_client_python\n",
    "```\n",
    "\n",
    "Navigate to the GROBID Python client directory:\n",
    "```\n",
    "cd grobid_client_python/\n",
    "```\n",
    "\n",
    "Install the GROBID Python client:\n",
    "```\n",
    "python3 setup.py install\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c19f5493",
   "metadata": {},
   "source": [
    "## Run GROBID on Nested Directories"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fd518b1",
   "metadata": {},
   "source": [
    "Create a Bash script, pdfextractionGrobid.sh, to process all PDF files in nested directories:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5809d042",
   "metadata": {},
   "source": [
    "```\n",
    "#!/bin/bash\n",
    "\n",
    "# Base directory\n",
    "base_dir=\"~/install/grobid_client_python/pdf\"\n",
    "\n",
    "# Find all directories in the base directory and its subdirectories\n",
    "find \"${base_dir}\" -type d | while read dir; do\n",
    "    # Process all the PDFs in the directory with GROBID\n",
    "    grobid_client --input \"${dir}\" --output \"${dir}\" processFulltextDocument\n",
    "done\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da40fd87",
   "metadata": {},
   "source": [
    "Make the script executable and run it:\n",
    "```\n",
    "chmod +x pdfextractionGrobid.sh\n",
    "./pdfextractionGrobid.sh\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5d11435",
   "metadata": {},
   "source": [
    "## Replicate Directory Structure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d41c464",
   "metadata": {},
   "source": [
    "To replicate the directory structure from one location to another \n",
    "(for example, from data/xml/ to data/partitions/), you can use the rsync command:\n",
    "\n",
    "```\n",
    "rsync -av -f\"+ */\" -f\"- *\" data/xml/ data/partitions/\n",
    "```\n",
    "\n",
    "This command will create an identical directory structure in data/partitions/ without copying the files.\n",
    "\n",
    "By following these steps, you can use GROBID to automatically extract structured data from scientific PDFs in nested directories. This data can then be processed and analyzed further, as demonstrated in the main part of this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02a6ce7c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
