{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "58c98e55",
   "metadata": {},
   "source": [
    "# Open Source Vector Database for LLM local Memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80a69b79",
   "metadata": {},
   "source": [
    "-------------------------------------------------------------\n",
    "Notebook created by Marcelo Amaral.\n",
    "Notebook developed with assistance from GPT (July 2023). Designed to interface with a local Milvus database setup. Ensure Milvus is configured and running locally before execution.\n",
    "-------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3598828",
   "metadata": {},
   "source": [
    "The idea is to put a system in production that leverages large language models for our specific research.\n",
    "\n",
    "Here's a rough step-by-step this tutorial aims to do:\n",
    "\n",
    "    Extract Text from LaTeX: Use a library like text_splitter, TexSoup or pylatexenc to extract and clean text from your LaTeX papers and other documents.\n",
    "\n",
    "    Embed: Use Setence Transformers to generate embeddings for the extracted text.\n",
    "\n",
    "    Store in Milvus: Insert the generated embeddings into a Milvus collection for storage and retrieval. Remember to also store any necessary metadata (like paper IDs or partition information) so you can associate embeddings with their corresponding papers.\n",
    "\n",
    "    Semantic Search: Use Milvus to perform semantic searches. Given a query, convert it to an embedding using the same method as in step 2, and then query Milvus to find the most similar embeddings in the database.\n",
    "\n",
    "    Reasoning with GPT: Once you have the search results, you can pass the corresponding text to a GPT model for further processing. The idea will be to have a plugin."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b3ea04e",
   "metadata": {},
   "source": [
    "## Install and Admin Milvus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "164ba52f",
   "metadata": {},
   "source": [
    "Check the requirements for your system here: https://milvus.io/docs/prerequisite-docker.md (visited Jul 10, 2023)\n",
    "\n",
    "I will install in linux but should be similar for other systems: https://milvus.io/docs/install_standalone-docker.md"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e54c11b",
   "metadata": {},
   "source": [
    "For debian based linux we need to install Docker and Docker Compose\n",
    "\n",
    "    sudo apt-get update\n",
    "    sudo apt-get upgrade\n",
    "    sudo apt-get install docker-compose"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c8986c5",
   "metadata": {},
   "source": [
    "Or the docker engine with docker compose plugin\n",
    "\n",
    "https://docs.docker.com/engine/install/ubuntu/#installation-methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62443d27",
   "metadata": {},
   "source": [
    "    sudo apt-get install docker-ce docker-ce-cli containerd.io docker-buildx-plugin docker-compose-plugin docker-compose-plugin\n",
    " \n",
    "Then the commands below will be without \"-\", for example:\n",
    "\n",
    "    docker compose up -d"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0af2c92b",
   "metadata": {},
   "source": [
    "Download the YAML file\n",
    "\n",
    "    wget https://github.com/milvus-io/milvus/releases/download/v2.2.11/milvus-standalone-docker-compose.yml -O docker-compose.yml\n",
    "\n",
    "Start Milvus\n",
    "\n",
    "In the same directory as the docker-compose.yml file, start up Milvus by running:\n",
    "\n",
    "    sudo docker-compose up -d\n",
    "\n",
    "Check if the containers are up and running.\n",
    "\n",
    "    sudo docker-compose ps\n",
    "\n",
    "Connect to Milvus\n",
    "\n",
    "Verify which local port the Milvus server is listening on. Replace the container name with your own.\n",
    "       \n",
    "    sudo docker port milvus-standalone 19530/tcp\n",
    "\n",
    "Stop Milvus\n",
    "\n",
    "To stop Milvus standalone, run:\n",
    "\n",
    "    sudo docker-compose down\n",
    "\n",
    "To delete data after stopping Milvus, run:\n",
    "\n",
    "    sudo rm -rf  volumes\n",
    "\n",
    "\n",
    "To see container ID:\n",
    "\n",
    "    sudo docker ps -a\n",
    "\n",
    "To login inside the container:\n",
    "\n",
    "    sudo docker exec -u 0 -it yourcontainer /bin/bash\n",
    "\n",
    "To install a text editor inside the container:\n",
    "\n",
    "    apt-get update\n",
    "    apt-get intall nano\n",
    "\n",
    "To update IP:\n",
    "\n",
    "Edit docker-compose.yml\n",
    "\n",
    "ports:\n",
    "\n",
    "      - \"x.xx.xx.xx:19530:19530\"\n",
    "      - \"x.xx.xx.xx:9091:9091\"\n",
    "\n",
    "And In my case I had in the end of the file just, not need to create a specific network: \n",
    "\n",
    "    networks:    \n",
    "     default:\n",
    "\n",
    "Then restart the container\n",
    "\n",
    "    docker-compose down\n",
    "    docker-compose up -d\n",
    "\n",
    "\n",
    "Now we can connect with python, for example:\n",
    "      \n",
    "    connections.connect(\"default\", host=\"xx.xx.xx.xx\", port=\"19530\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "435f8230",
   "metadata": {},
   "source": [
    "Currently to intall pymilvus in a client it is required to install together this two pre-requisites:\n",
    "pip install protobuf==3.20.0 grpcio-tools \n",
    "then\n",
    "pip install pymilvus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98bca8eb",
   "metadata": {},
   "source": [
    "## Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0f03793d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pymilvus import (\n",
    "    connections,\n",
    "    utility,\n",
    "    FieldSchema, CollectionSchema, DataType,\n",
    "    Collection,\n",
    ")\n",
    "# The powerfull transformers models\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "818e00b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'packages.qgr_data_processing' from '/home/mamaral/Documents/qgr/codes/python/notebooks/computational_essays/packages/qgr_data_processing.py'>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# In case you need to work updte the modeule. This imports the importlib module, \n",
    "# which contains functions that help you control \n",
    "# the runtime process of Python scripts, especially those related to importing and reloading modules.\n",
    "import importlib\n",
    "\n",
    "# Then imports the youtube_data_processing module under the alias yt.\n",
    "from packages import qgr_data_processing as qgr\n",
    "\n",
    "# This reloads the youtube_data module. The purpose of this is to ensure that the \n",
    "# latest version of the module is in use, especially if the module has been modified \n",
    "# since the start of the Python session.\n",
    "importlib.reload(qgr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97a50f0a",
   "metadata": {},
   "source": [
    "## Connecting and managing the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1dde8e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# configure /milvus/configs/milvus.yaml with your IP host and change and uncomment here\n",
    "connections.connect(\"default\", host=\"192.168.1.90\", port=\"19530\") \n",
    "#connections.connect(\"default\", host=\"localhost\", port=\"19530\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b4eac923",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Existing collections: ['YT_Videos', 'QGRmemory']\n"
     ]
    }
   ],
   "source": [
    "# List existing collections (this will return an empty list if no collections exist)\n",
    "collections = utility.list_collections()\n",
    "print(\"Existing collections:\", collections)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59499533",
   "metadata": {},
   "source": [
    "### tables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93f9d60f",
   "metadata": {},
   "source": [
    "The function create_milvus_collection_with_partitions creates a Milvus collection with a specified name and dimensionality for the vector field, and with specific partitions.\n",
    "\n",
    "It takes three arguments:\n",
    "\n",
    "    collection_name: the name of the collection to be created.\n",
    "    dim: the dimensionality of the vector field.\n",
    "    partition_names: a list of partition names to be created within the collection.\n",
    "\n",
    "The function begins by checking if a collection with the given name already exists. If it does, the existing collection is dropped.\n",
    "\n",
    "Then, it defines the schema for the collection. The schema includes an ID field, two VARCHAR fields for the title and content of the documents, and a FLOAT_VECTOR field for the document embeddings. The ID field is marked as the primary field and is set to auto-generate IDs.\n",
    "\n",
    "After defining the schema, the function creates a new collection with the given name and schema.\n",
    "\n",
    "The function then creates partitions within the collection. For each name in the partition_names list, it creates a partition with that name.\n",
    "\n",
    "Once the collection and partitions are set up, the function creates an index on the content_vector field to speed up similarity searches. The type of the index is IVF_FLAT, and the similarity metric is L2 (Euclidean distance).\n",
    "\n",
    "Finally, the function returns the created collection.\n",
    "\n",
    "When inserting data into the collection, you can specify the partition to insert into. This allows you to keep related data together and can improve search performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "91211494",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Status(code=0, message=)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Define the schema for the new collection\n",
    "fields = [\n",
    "    FieldSchema(name=\"id\", dtype=DataType.INT64, is_primary=True, auto_id=True),\n",
    "    FieldSchema(name=\"documentId\", dtype=DataType.VARCHAR, max_length=256, auto_id=False),\n",
    "    FieldSchema(name=\"title\", dtype=DataType.VARCHAR, max_length=1024), \n",
    "    FieldSchema(name=\"date\", dtype=DataType.VARCHAR, max_length=256),\n",
    "    FieldSchema(name=\"authors\", dtype=DataType.VARCHAR, max_length=1024), \n",
    "    FieldSchema(name=\"abstract\", dtype=DataType.VARCHAR, max_length=4096), \n",
    "    FieldSchema(name=\"keywords\", dtype=DataType.VARCHAR, max_length=1024), \n",
    "    FieldSchema(name=\"category\", dtype=DataType.VARCHAR, max_length=256),\n",
    "    FieldSchema(name=\"content\", dtype=DataType.VARCHAR, max_length=1024),   \n",
    "    FieldSchema(name=\"content_vector\", dtype=DataType.FLOAT_VECTOR, dim=384)\n",
    "]\n",
    "collection_name = \"QGRmemory\"\n",
    "\n",
    "##Drop the collection if it already exists\n",
    "#if utility.has_collection(collection_name):\n",
    "#    utility.drop_collection(collection_name)\n",
    "\n",
    "# List of partition names.\n",
    "#partition_names = ['mypapers', 'papers', 'notes', 'books', 'others', 'chats']\n",
    "\n",
    "# Create the collection schema\n",
    "data_schema = CollectionSchema(fields=fields, description='QGR Content Data')\n",
    "\n",
    "# Create the collection\n",
    "collection = Collection(name=collection_name, schema=data_schema)\n",
    "\n",
    "# List of partition names.\n",
    "partition_names = ['mypapers', 'papers', 'notes', 'books', 'others', 'chats']\n",
    "\n",
    "# Create the partitions\n",
    "for partition_name in partition_names:\n",
    "    collection.create_partition(partition_name)\n",
    "\n",
    "# Create a vector index for semantic search\n",
    "index_params = {\n",
    "    'metric_type': \"IP\", #other option L2\n",
    "    'index_type': \"IVF_FLAT\",\n",
    "    'params': {\"nlist\": 2048}\n",
    "}\n",
    "\n",
    "collection.create_index(field_name='content_vector', index_params=index_params)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62759429",
   "metadata": {},
   "source": [
    "Since we' will be using the sentence-transformers/multi-qa-MiniLM-L6-cos-v1 model to generate embeddings and we are normalizing them, it would indeed make sense to use a cosine similarity metric for our index instead of the L2 metric. Inner Product (IP) similarity metric can be used in conjunction with normalized vectors to achieve cosine similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c1d3e83",
   "metadata": {},
   "source": [
    "## Preparing and Inserting Data "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da3f9530",
   "metadata": {},
   "source": [
    "Choosing the best model for your text embedding task can depend on the specifics of your use case. For tasks involving academic papers or scientific texts, the following models might be suitable:\n",
    "\n",
    "    SciBERT: A variant of BERT pre-trained on a large corpus of scientific texts. It may perform better on scientific texts due to its exposure to scientific jargon and concepts.\n",
    "    BioBERT: Tailored for biology papers, this BERT model is pre-trained on a large-scale biomedical corpus, making it adept at handling biomedical terminology.\n",
    "    MathBERT: A BERT variant specifically designed for mathematical papers.\n",
    "    GPT-4 or GPT-3: OpenAI's general-purpose models, which can be powerful for various language understanding and generation tasks.\n",
    "    LaBSE (Language-agnostic BERT Sentence Embeddings): Ideal for multilingual sentence-level embeddings, this model is particularly useful for cross-lingual tasks.\n",
    "    sentence_transformers: A library offering models like multi-qa-MiniLM-L6-cos-v1 and paraphrase-MiniLM-L6-v2, optimized for sentence embeddings.\n",
    "    multi-qa-MiniLM-L6-cos-v1: Proven effective for embedding in other tests, this model may also be suitable for other contexts.\n",
    "\n",
    "Given the specific needs of your database, multi-qa-MiniLM-L6-cos-v1 might be a good choice to try."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "96ef324f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flushed and saved processed files at 20\n",
      "Flushed and saved processed files at 28\n"
     ]
    }
   ],
   "source": [
    "# Assuming already connected to Milvus\n",
    "collection_name = \"QGRmemory\"\n",
    "collection = Collection(name=collection_name)\n",
    "\n",
    "# Partition name for the data\n",
    "partition_name = \"mypapers\"\n",
    "\n",
    "#logging.basicConfig(level=logging.ERROR)\n",
    "\n",
    "# Load the pre-trained SBERT model\n",
    "sbert_model = SentenceTransformer('sentence-transformers/multi-qa-MiniLM-L6-cos-v1') \n",
    "\n",
    "# Initialize a list to keep track of processed file paths\n",
    "processed_file_name = \"processed_files.json\"\n",
    "processed_files = qgr.load_processed_files(processed_file_name)\n",
    "\n",
    "#Folders to process:\n",
    "folder_path=\"mypapers\"\n",
    "folder_path_not_processed = \"notprocessed\"\n",
    "files_processed = 0\n",
    "files_process_max = 40\n",
    "while files_processed < files_process_max:\n",
    "    try:\n",
    "        entry, file_path = qgr.process_file_from_folder(folder_path)\n",
    "    except Exception as e: # Catch the specific exception and print it\n",
    "        print(f\"error folder_path: {folder_path}\")\n",
    "        print(f\"Error details: {str(e)}\")\n",
    "        break\n",
    "        \n",
    "    if entry is None:\n",
    "        break\n",
    "\n",
    "    try:\n",
    "        qgr.process_and_insert_documents(entry, sbert_model, collection, partition_name)\n",
    "\n",
    "        # List of partition names.\n",
    "        #partition_names = ['mypapers', 'papers', 'notes', 'books', 'other']\n",
    "       \n",
    "        # Delete the file after insertion\n",
    "        os.remove(file_path)\n",
    "        \n",
    "        # Append the successfully processed file path to the list\n",
    "        processed_files.append(file_path)\n",
    "        \n",
    "        #print(file_path)\n",
    "        files_processed += 1  # Increment the counter\n",
    "        \n",
    "        # Flush the data and save the processed file paths every 1000 files\n",
    "        if files_processed % 20 == 0:\n",
    "            collection.flush()\n",
    "            with open(processed_file_name, 'w') as json_file:\n",
    "                json.dump(processed_files, json_file)\n",
    "            print(f\"Flushed and saved processed files at {files_processed}\")\n",
    "\n",
    "        \n",
    "    except Exception as e: # Catch the specific exception and print it\n",
    "        print(f\"file not fully processed: {file_path}\")\n",
    "        print(f\"Error details: {str(e)}\")\n",
    "        \n",
    "        # Move the file to the \"notprocessed\" folder\n",
    "        destination_path = os.path.join(folder_path_not_processed, os.path.basename(file_path))\n",
    "        shutil.move(file_path, destination_path)\n",
    "        \n",
    "# Flush the data\n",
    "collection.flush()\n",
    "print(f\"Flushed and saved processed files at {files_processed}\")\n",
    "\n",
    "# Save the updated list of processed file paths to the JSON file\n",
    "with open(processed_file_name, 'w') as json_file:\n",
    "    json.dump(processed_files, json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "4c2fd2a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flush the data\n",
    "collection.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a33ff7e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of entities in the collection is: 392712\n"
     ]
    }
   ],
   "source": [
    "# Assuming already connected to Milvus\n",
    "collection_name = \"QGRmemory\"\n",
    "collection = Collection(name=collection_name)\n",
    "\n",
    "num_entities = collection.num_entities\n",
    "print(f\"The number of entities in the collection is: {num_entities}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "277e43f7",
   "metadata": {},
   "source": [
    "## Reading Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c889f31a",
   "metadata": {},
   "source": [
    "About the search:\n",
    "\n",
    "    limit: This parameter determines the number of returned results. Depending on your use case, you might want to return more results for further processing, or fewer results for speed and simplicity.\n",
    "\n",
    "    expr: This parameter allows you to filter the results based on conditions. If you have other fields in your collection that could help refine your search results, you can use this parameter to add conditions. For example, if you have a \"date\" field and you only want papers from the last five years, you could set expr=\"date > '2018'\".\n",
    "\n",
    "    output_fields: Here you specify additional fields that you want to retrieve for each result. You've already added \"title\" and \"content\", but you can add more fields if they are defined in your collection.\n",
    "\n",
    "    partition_names: You're already searching within the \"mypapers\" partition. If you want to search within multiple partitions, you can add more partition names to the list."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "824ee1b9",
   "metadata": {},
   "source": [
    "About refining for better search results.\n",
    "\n",
    "The nlist parameter is used when building the index and influences the accuracy of the vector insertion. A higher nlist can make the vector insertion more precise but will increase the index file size and the time it takes to create the index.\n",
    "\n",
    "On the other hand, nprobe is used during the search process. It determines the number of clusters to inspect during the search. A higher nprobe can increase the recall rate of a search but will also increase the search time.\n",
    "\n",
    "In practice, it's common to set nlist to a relatively high value (to ensure good indexing accuracy) and then adjust nprobe depending on the speed-accuracy trade-off you're willing to accept for your searches.\n",
    "\n",
    "Query Refinement: Make sure your query is representative of the information you are looking for. Since SciBERT is a BERT-based model, it might be sensitive to the phrasing and choice of words in the query.\n",
    "\n",
    "Check Data Preprocessing: Make sure that the text preprocessing for both your dataset and search queries are appropriate and consistent. Ensure that the LaTeX text is being correctly parsed and cleaned before being passed to the SciBERT model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "39738050",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f1d1f9cb9f040999c51360982ae30bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'54b8943ededb63c826aa061b9767067cc31f09162778c3581365f8828bef4503': {'title': 'QUASICRYSTALLINE SPIN FOAM WITH MATTER: DEFINITIONS AND EXAMPLES', 'date': 'Unknown', 'authors': 'Marcelo Amaral, Richard Clawson, Klee Irwin', 'abstract': 'In this work, we define quasicrystalline spin networks as a subspace within the standard Hilbert space of loop quantum gravity, effectively constraining the states to coherent states that align with quasicrystal geometry structures. We introduce quasicrystalline spin foam amplitudes, a variation of the EPRL spin foam model, in which the internal spin labels are constrained to correspond to the boundary data of quasicrystalline spin networks. Within this framework, the quasicrystalline spin foam amplitudes encode the dynamics of quantum geometries that exhibit aperiodic structures. Additionally, we investigate the coupling of fermions within the quasicrystalline spin foam amplitudes. We present calculations for three-dimensional examples and then explore the 600-cell construction, which is a fundamental component of the four-dimensional Elser-Sloane quasicrystal derived from the E8 root lattice.', 'keywords': 'Unknown', 'category': 'mypapers', 'contents': ['\\\\title{QUASICRYSTALLINE SPIN FOAM WITH MATTER: DEFINITIONS AND EXAMPLES}\\n\\\\author{Marcelo Amaral}\\n\\\\author{Richard Clawson}\\n\\\\author{Klee Irwin}\\n\\\\begin{abstract}\\nIn this work, we define quasicrystalline spin networks as a subspace within the standard Hilbert space of loop quantum gravity, effectively constraining the states to coherent states that align with quasicrystal geometry structures. We introduce quasicrystalline spin foam amplitudes, a variation of the EPRL spin foam model, in which the internal spin', 'model for quasicrystals and implement QSF for the icosahedron and the octahedron, which are building blocks of known 3D quasicrystals. We show that in the large spin limit, it is in accordance with the expected power law behavior from analytical results, indicating that this kind of geometry dominates the amplitude.\\nIn 4D, we study quasicrystalline EPRL spin foam amplitudes, particularly focusing on the 600-cell polytope, a building block of the more known 4D quasicrystal, the Elser-Sloane quasicrystal'], 'score': 0.7376246452331543}, '929f6ca17ae99286a419e19f317771dada8bcdc179265136960868ebe2b9a95f': {'title': 'Quasicrystalline Spin Foam with Matter: Definitions and Examples', 'date': '2023', 'authors': 'Marcelo Amaral, Richard Clawson and Klee Irwin', 'abstract': 'In this work, we define quasicrystalline spin networks as a subspace within the standard Hilbert space of loop quantum gravity, effectively constraining the states to coherent states that align with quasicrystal geometry structures. We introduce quasicrystalline spin foam amplitudes, a variation of the EPRL spin foam model, in which the internal spin labels are constrained to correspond to the boundary data of quasicrystalline spin networks. Within this framework, the quasicrystalline spin foam amplitudes encode the dynamics of quantum geometries that exhibit aperiodic structures. Additionally, we investigate the coupling of fermions within the quasicrystalline spin foam amplitudes. We present calculations for three-dimensional examples and then explore the 600-cell construction, which is a fundamental component of the four-dimensional Elser-Sloane quasicrystal derived from the E8 root lattice.', 'keywords': 'Unknown', 'category': 'mypapers', 'contents': ['\\\\title{Quasicrystalline Spin Foam with Matter: Definitions and Examples}\\n\\\\author{Marcelo Amaral, Richard Clawson and Klee Irwin}\\n\\n\\\\begin{abstract}\\nIn this work, we define quasicrystalline spin networks as a subspace within the standard Hilbert space of loop quantum gravity, effectively constraining the states to coherent states that align with quasicrystal geometry structures. We introduce quasicrystalline spin foam amplitudes, a variation of the EPRL spin foam model, in which the internal spin labels are'], 'score': 0.7347844243049622}}\n"
     ]
    }
   ],
   "source": [
    "# Load the pre-trained SBERT model\n",
    "sbert_model = SentenceTransformer('sentence-transformers/multi-qa-MiniLM-L6-cos-v1') \n",
    "query_text = \"quasicrystals and spin foams\"\n",
    "results = qgr.search_documents(query_text, \"QGRmemory\", sbert_model, \"mypapers\", limit=3)\n",
    "grouped_results = qgr.group_by_document_id(results)\n",
    "print(grouped_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a3dabe8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "#Deleting\n",
    "# Step 1: Search for the documentId to get the primary key (id)\n",
    "collection_name = \"QGRmemory\"\n",
    "collection = Collection(name=collection_name)\n",
    "\n",
    "document_id_to_search = \"54b8943ededb63c826aa061b9767067cc31f09162778c3581365f8828bef4503\"\n",
    "\n",
    "search_results = collection.query(f\"documentId == '{document_id_to_search}'\")\n",
    "print(search_results)\n",
    "\n",
    "# Step 2: Extract the primary keys from the search results\n",
    "primary_keys_to_delete = [result['id'] for result in search_results]\n",
    "\n",
    "# Step 3: Delete the entities using the primary keys\n",
    "delete_expr = f\"id in {primary_keys_to_delete}\"\n",
    "#collection.delete(delete_expr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "0f050f51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Status(code=0, message=)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Re-indexing\n",
    "#Release the collection\n",
    "collection.release()\n",
    "#Drop the existing index\n",
    "collection.drop_index()\n",
    "#Create a new index\n",
    "index_params = {\n",
    "    'metric_type': \"IP\",\n",
    "    'index_type': \"IVF_FLAT\",\n",
    "    'params': {\"nlist\": 2048}\n",
    "}\n",
    "collection.create_index(field_name='content_vector', index_params=index_params)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
